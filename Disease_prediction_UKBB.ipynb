{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Disease prediction based on genetic and lifestyle information**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Split phenotype information (.csv file) into .npy files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import csv\n",
    "import sys\n",
    "csv.field_size_limit(sys.maxsize)\n",
    "\n",
    "NUM = 502628 # number of individuals\n",
    "\n",
    "def save_data(dir_save, names, data):\n",
    "    if not os.path.exists(dir_save):\n",
    "        os.makedirs(dir_save)\n",
    "    if type(names) is list:\n",
    "        for i, name in enumerate(names):\n",
    "            np.save(os.path.join(dir_save, str(name)), data[i])\n",
    "    else:\n",
    "        np.save(os.path.join(dir_save, str(names)), data)\n",
    "    \n",
    "def get_data(dir_file, names):\n",
    "    # Get the data of item names for all individuals\n",
    "    # return the data list if names is char; a list containing data lists if names is a list\n",
    "    \n",
    "    if type(names) is list:\n",
    "        data = [[] for i in names]\n",
    "    else:\n",
    "        data = []\n",
    "    \n",
    "    inds = names # get_ind(dir_file, names)\n",
    "        \n",
    "    with open(dir_file, 'r') as csvfile:\n",
    "        reader = csv.reader(csvfile, delimiter=',')\n",
    "        for i, row in enumerate(reader):\n",
    "            if np.mod(i, 10000) == 0:\n",
    "                print(i)\n",
    "            if type(names) is list:\n",
    "                for j, ind in enumerate(inds):\n",
    "                    #print(str(len(row))+'\\t'+str(ind))\n",
    "                    data[j].append(row[ind])\n",
    "            else:\n",
    "                data.append(row[inds])\n",
    "    return data\n",
    "\n",
    "def generate_data(dir_file, dir_save, names):\n",
    "    names_new = []\n",
    "    # only generate data which are not generated before\n",
    "    for name in names:\n",
    "        if not os.path.isfile(os.path.join(dir_save, str(name)) + '.npy'):\n",
    "            names_new.append(name)\n",
    "            \n",
    "    # if all items are generated before\n",
    "    if not names_new:\n",
    "        return\n",
    "    data_new = get_data(dir_file, names_new)\n",
    "    save_data(dir_save, names_new, data_new)\n",
    "\n",
    "names=[]\n",
    "dir_save = '/oak/stanford/groups/arend/Eric/UKBB/phenotype/phenotype_13721/'\n",
    "dir_file = '/scratch/PI/eriking/ukb/app1372/processed/ukb9430.csv/'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step2: Identify white british participants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_person_id=np.load('/oak/stanford/groups/arend/Eric/UKBB/phenotype_13721/0.npy')\n",
    "col_6669=np.load('/oak/stanford/groups/arend/Eric/UKBB/phenotype_13721/6669.npy')\n",
    "col_6670=np.load('/oak/stanford/groups/arend/Eric/UKBB/phenotype_13721/6670.npy')\n",
    "col_6671=np.load('/oak/stanford/groups/arend/Eric/UKBB/phenotype_13721/6671.npy')\n",
    "#merge three columns and filter first column\n",
    "individual=all_person_id[1:]\n",
    "ethnics=col_6669[1:]\n",
    "for i in range(502628):\n",
    "    if ethnics[i]=='' and col_6670[i+1]!='':\n",
    "        ethnics[i]=col_6670[i+1]\n",
    "for i in range(502628):\n",
    "    if ethnics[i]=='' and col_6671[i+1]!='':\n",
    "        ethnics[i]=col_6671[i+1]\n",
    "outfile=open('/oak/stanford/groups/jamesz/eric/extract_ind','w')\n",
    "for i in range(502628):\n",
    "    if ethnics[i]=='1001' or ethnics[i]=='2001' or ethnics[i]=='3001' or ethnics[i]=='4001':\n",
    "        outfile.write(individual[i]+'\\t'+individual[i]+'\\n')\n",
    "outfile.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step3: Extract individuals by using Plink\n",
    "\n",
    "**plink --bfile allsamples --remove white_withdraw.ind --mak-bed --out merge_white_Britich_clean**\n",
    "\n",
    "where white_withdraw.ind include the sample id in extract_ind and the participants decided to withdraw their data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Extract 65 lifestyles and environment features (L&E) from .csv file.\n",
    "\n",
    "The L&E information was included in two files, Raw_pheno_final.matrix and Raw_pheno_final.info: \n",
    "\n",
    "1. Raw_pheno_final.matrix: each row represents one L&E feature.\n",
    "2. Raw_pheno_final.info: has two columns (feature type (Categorical, Integer or Continuous) and feature name)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Impute missing valules of L&E features\n",
    "\n",
    "Raw_pheno_final_impute.matrix and Raw_pheno_final_impute.info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import Imputer\n",
    "import numpy as np\n",
    "#input missing value in phenotype data\n",
    "infile1=open('Raw_pheno_final.matrix','r')\n",
    "infile2=open('Raw_pheno_final.info','r')\n",
    "outfile1=open('Raw_pheno_final_impute.matrix','w')\n",
    "outfile2=open('Raw_pheno_final_impute.info','w')\n",
    "datatype=[]\n",
    "datainfo=[]\n",
    "for line in infile2:\n",
    "    datatype.append(line.strip(' \\n').split('\\t')[0])\n",
    "    datainfo.append(line)\n",
    "index=0\n",
    "for line in infile1:\n",
    "    print(index)\n",
    "    A=line.strip(' \\n').split(' ')\n",
    "    print(len(A))\n",
    "    if datatype[index]=='Integer' or datatype[index]=='Continuous':\n",
    "        missing=0\n",
    "        newdata=[]\n",
    "        for d in A:\n",
    "            if d=='NAN':\n",
    "                missing+=1\n",
    "                newdata.append(111111)\n",
    "            elif float(d)<0:\n",
    "                missing+=1\n",
    "                newdata.append(111111)\n",
    "            else:\n",
    "                newdata.append(float(d))\n",
    "        if missing>50000:\n",
    "            index+=1\n",
    "            continue\n",
    "        else:\n",
    "            imputer=Imputer(missing_values=111111, strategy='median', axis=0, verbose=0, copy=True)\n",
    "            X=imputer.fit_transform(np.array(newdata).reshape(-1,1))\n",
    "            for x in X:\n",
    "                outfile1.write(str(x[0])+' ')\n",
    "            outfile1.write('\\n')\n",
    "            outfile2.write(datainfo[index])\n",
    "    elif 'Categorical' in datatype[index]:\n",
    "        missing=0\n",
    "        newdata=[]\n",
    "        for d in A:\n",
    "            if d=='NAN':\n",
    "                missing+=1\n",
    "                newdata.append(111111)\n",
    "            elif float(d)<0:\n",
    "                missing+=1\n",
    "                newdata.append(111111)\n",
    "            else:\n",
    "                newdata.append(float(d))\n",
    "        if missing>50000:\n",
    "            index+=1\n",
    "            continue\n",
    "        else:\n",
    "            imputer=Imputer(missing_values=111111, strategy='most_frequent', axis=0, verbose=0, copy=True)\n",
    "            X=imputer.fit_transform(np.array(newdata).reshape(-1,1))\n",
    "            for x in X:\n",
    "                outfile1.write(str(x[0])+' ')\n",
    "            outfile1.write('\\n')\n",
    "            outfile2.write(datainfo[index])\n",
    "    index+=1\n",
    "infile1.close()\n",
    "infile2.close()\n",
    "outfile1.close()\n",
    "outfile2.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Convert Categorical features into dummy variables "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "infile_data=open('Raw_pheno_final_impute.matrix','r')\n",
    "infile_info=open('Raw_pheno_final_impute.info','r')\n",
    "description=[]\n",
    "datatype=[]\n",
    "for line in infile_info:\n",
    "    A=line.strip(' \\n').split('\\t')\n",
    "    description.append(A[1])\n",
    "    datatype.append(A[0])\n",
    "infile_info.close()\n",
    "outfile_result=open('Raw_pheno_final_impute_dummy.matrix','w')\n",
    "outfile_label=open('Raw_pheno_final_impute_dummy.info','w')\n",
    "\n",
    "index=0\n",
    "for line in infile_data:\n",
    "    print(index)\n",
    "    A=line.strip(' \\n').split(' ')\n",
    "    print(len(A))\n",
    "    mylist = list(set(A))\n",
    "    if len(mylist)==1:\n",
    "        index+=1\n",
    "        continue\n",
    "    if 'Categorical' in datatype[index]:\n",
    "        if len(mylist)==2:\n",
    "            outfile_label.write(description[index]+'\\n')\n",
    "            outfile_result.write(line)\n",
    "        else:\n",
    "            df = pd.DataFrame(data=np.asarray(A).transpose(),columns=['data'])\n",
    "            just_dummies = pd.get_dummies(df['data'])\n",
    "            step_1 = pd.concat([df, just_dummies], axis=1)\n",
    "            step_1.drop(['data'], inplace=True, axis=1)\n",
    "            step_1 = step_1.applymap(np.int)\n",
    "            Y=step_1.columns\n",
    "            X=np.asarray(step_1).transpose()\n",
    "            sizelabel=X.shape[0]\n",
    "            print(sizelabel)\n",
    "            for i in range(sizelabel):\n",
    "                outfile_label.write(description[index]+'\\t'+str(Y[i])+'\\n')\n",
    "                outfile_result.write(\" \".join(map(str,X[i])))\n",
    "                outfile_result.write('\\n')\n",
    "    else:\n",
    "        outfile_label.write(description[index]+'\\n')\n",
    "        outfile_result.write(line)\n",
    "    index+=1\n",
    "outfile_result.close()\n",
    "infile_data.close()\n",
    "outfile_label.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step7: Generate information of self-reported cancers/diseases\n",
    "\n",
    "All the self-reported cancers/diseases should be first grouped into the second level of the disease tree structure according to [Data-Coding 3](https://biobank.ctsu.ox.ac.uk/crystal/coding.cgi?id=3) (cancers) and [Data-Coding 6](https://biobank.ctsu.ox.ac.uk/crystal/coding.cgi?id=6) (diseases) \n",
    "\n",
    "Next, we generated .data and .list files for self-reported cancers and diseases, respectively. And only kept the cancers/diseases whose sample size were larger than 6,000\n",
    "\n",
    "1. .list files include two columns: disease id and the number of patients. \n",
    "\n",
    "2. .data files include the affection status, each row represent one disease/cancer(with the same order as in .list), where 1: affected; 0: unaffected.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "outdata1=open('/oak/stanford/groups/arend/Eric/UKBB/Genenvironment/self_cancer.data','w')\n",
    "outdata2=open('/oak/stanford/groups/arend/Eric/UKBB/Genenvironment/self_disease.data','w')\n",
    "\n",
    "outlist1=open('/oak/stanford/groups/arend/Eric/UKBB/Genenvironment/self_cancer.list','w')\n",
    "outlist2=open('/oak/stanford/groups/arend/Eric/UKBB/Genenvironment/self_disease.list','w')\n",
    "\n",
    "diseaseid_exit=[]\n",
    "cancer=defaultdict(list)\n",
    "disease=defaultdict(list)\n",
    "\n",
    "for i in range(4136,4154):\n",
    "    col=np.load('/oak/stanford/groups/arend/Eric/UKBB/phenotype_13721/'+str(i)+'.npy')\n",
    "    for j in range(1,502629):\n",
    "        if col[j]!='' and col[j]!='99999':\n",
    "            if cancer_map[col[j]]=='':\n",
    "                print(col[j])\n",
    "            cancer[cancer_map[col[j]]].append(j-1)\n",
    "    \n",
    "for i in range(4154,4241):\n",
    "    col=np.load('/oak/stanford/groups/arend/Eric/UKBB/phenotype_13721/'+str(i)+'.npy')\n",
    "    for j in range(1,502629):\n",
    "        if col[j]!='' and col[j]!='99999':\n",
    "            if disease_map[col[j]]=='':\n",
    "                print(col[j])\n",
    "            disease[disease_map[col[j]]].append(j-1)\n",
    "print('finished')\n",
    "\n",
    "for key,value in cancer.items():\n",
    "    sum_num=0\n",
    "    for i in range(502628):\n",
    "        if i in value:\n",
    "            sum_num+=1\n",
    "            outdata1.write('1'+'\\t')\n",
    "        else:\n",
    "            outdata1.write('0'+'\\t')\n",
    "    outdata1.write('\\n')\n",
    "    outlist1.write(key+'\\t')\n",
    "    outlist1.write(str(sum_num)+'\\n')\n",
    "outdata1.close()\n",
    "outlist1.close()\n",
    "    \n",
    "\n",
    "for key,value in disease.items():\n",
    "    sum_num=0\n",
    "    for i in range(502628):\n",
    "        if i in value:\n",
    "            sum_num+=1\n",
    "            outdata2.write('1'+'\\t')\n",
    "        else:\n",
    "            outdata2.write('0'+'\\t')\n",
    "    outdata2.write('\\n')\n",
    "    outlist2.write(key+'\\t')\n",
    "    outlist2.write(str(sum_num)+'\\n')\n",
    "    \n",
    "outdata2.close()\n",
    "outlist2.close()    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: Remove the inappropriate participants from the self-reported patients and L&E feature files.\n",
    "\n",
    "\n",
    "Only keep the participants in merge_white_Britich_clean.fam (generated in **Step 3**)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "all_person_id=np.load('/oak/stanford/groups/arend/Eric/UKBB/phenotype_13721/0.npy')\n",
    "infile=open('/oak/stanford/groups/jamesz/eric/genotype/merge_white_Britich_clean.fam','r')\n",
    "individual=all_person_id[1:].tolist()\n",
    "ind_genetics_index=[]\n",
    "for line in infile:\n",
    "    ind=line.split()[0]\n",
    "    ind_genetics_index.append(individual.index(ind))\n",
    "infile.close()\n",
    "infile_cancer=open('/oak/stanford/groups/arend/Eric/UKBB/Genenvironment/self_cancer.data','r')\n",
    "infile_disease=open('/oak/stanford/groups/arend/Eric/UKBB/Genenvironment/self_disease.data','r')\n",
    "outfile_cancer=open('/oak/stanford/groups/arend/Eric/UKBB/Genenvironment/self_cancer_sub.data','w')\n",
    "outfile_disease=open('/oak/stanford/groups/arend/Eric/UKBB/Genenvironment/self_disease_sub.data','w')\n",
    "outfile_cancer_num=open('/oak/stanford/groups/arend/Eric/UKBB/Genenvironment/cancer_num','w')\n",
    "outfile_disease_num=open('/oak/stanford/groups/arend/Eric/UKBB/Genenvironment/disease_num','w')\n",
    "\n",
    "for line in infile_cancer:\n",
    "    A=line.strip().split()\n",
    "    sum_num=0\n",
    "    for XX in ind_genetics_index:\n",
    "        outfile_cancer.write(A[XX]+'\\t')\n",
    "        sum_num=sum_num+int(A[XX])\n",
    "    outfile_cancer_num.write(str(sum_num)+'\\n')\n",
    "    outfile_cancer.write('\\n')\n",
    "\n",
    "for line in infile_disease:\n",
    "    A=line.strip().split()\n",
    "    sum_num=0\n",
    "    for XX in ind_genetics_index:\n",
    "        outfile_disease.write(A[XX]+'\\t')\n",
    "        sum_num=sum_num+int(A[XX])\n",
    "    outfile_disease_num.write(str(sum_num)+'\\n')\n",
    "    outfile_disease.write('\\n')\n",
    "infile_cancer.close()\n",
    "infile_disease.close()\n",
    "outfile_cancer.close()\n",
    "outfile_disease.close()\n",
    "outfile_cancer_num.close()\n",
    "outfile_disease_num.close()\n",
    "infile=open('Raw_pheno_final_impute_dummy.matrix','r')\n",
    "outfile=open('Raw_pheno_final_impute_dummy_sub.matrix','w')\n",
    "for line in infile:\n",
    "    A=line.strip().split()\n",
    "    for XX in ind_genetics_index:\n",
    "        outfile.write(A[XX]+'\\t')\n",
    "    outfile.write('\\n')\n",
    "infile.close()\n",
    "outfile.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step9: Generate information of the cancers/diseases from hospitalization records\n",
    "\n",
    "We generate disease_status and disease_info based on ICD 10 codes in hospitalization records. \n",
    "\n",
    "### disease_info has three columns: \n",
    "\n",
    ">1. ICD 10 code;\n",
    "\n",
    ">2. number of patients before baseline;\n",
    "\n",
    ">3. number of patients after baseline.\n",
    "\n",
    "### disease_status is a matrix, each ICD 10 code in disease_info has two rows. \n",
    "\n",
    ">1. The odd rows are the diagnosis outcomes for ICD 10 codes (the same order as disease_info), where -1: before baseline; 1: after baseline; 0: not affected; \n",
    "\n",
    ">2. the even rows represent the corresponding diagnosis time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "disease=defaultdict(disease_status)\n",
    "file=open('/scratch/PI/eriking/ukb/app1372/hes/app1372_dbtable_hesin_2017aug22.tsv','r')\n",
    "outfile_data=open('/oak/stanford/groups/arend/Eric/UKBB/phenotype_merge/disease_status','w')\n",
    "outfile_info=open('/oak/stanford/groups/arend/Eric/UKBB/phenotype_merge/disease_info','w')\n",
    "index=0\n",
    "originaldate=''\n",
    "for line in file:\n",
    "    if index==0:\n",
    "        index+=1\n",
    "        continue\n",
    "    A=line.strip('\\n').split('\\t')\n",
    "    B=A[3]\n",
    "    if B=='':\n",
    "        continue\n",
    "    if A[2]!='':\n",
    "        newdate = time.strptime(A[2], \"%Y-%m-%d\")\n",
    "        originaldate=A[2]\n",
    "    elif A[5]!='': \n",
    "        newdate = time.strptime(A[5], \"%Y-%m-%d\")\n",
    "        originaldate=A[5]\n",
    "    elif A[6]!='': \n",
    "        newdate = time.strptime(A[6], \"%Y-%m-%d\")\n",
    "        originaldate=A[6]\n",
    "    elif A[7]!='': \n",
    "        newdate = time.strptime(A[7], \"%Y-%m-%d\")\n",
    "        originaldate=A[7]\n",
    "    if A[0] in disease[B[0:3]].ind.keys():\n",
    "        olddate = disease[B[0:3]].ind[A[0]]\n",
    "        if  newdate< time.strptime(olddate, \"%Y-%m-%d\"):\n",
    "            disease[B[0:3]].ind[A[0]]=originaldate\n",
    "    else:\n",
    "        disease[B[0:3]].ind[A[0]]=originaldate\n",
    "    index+=1\n",
    "print('finish 1')\n",
    "for key,value in disease.items():\n",
    "    print(key)\n",
    "    outfile_info.write(key+'\\t')\n",
    "    new_patient=0\n",
    "    old_patient=0\n",
    "    X=[]\n",
    "    Y=[]\n",
    "    for i in range(1,502629):\n",
    "        if individual[i] in value.ind.keys():\n",
    "            Y.append(value.ind[individual[i]])\n",
    "            if time.strptime(ind_time[individual[i]], \"%Y-%m-%d\")<time.strptime(value.ind[individual[i]], \"%Y-%m-%d\"):\n",
    "                X.append('1')\n",
    "                new_patient+=1\n",
    "            else:\n",
    "                old_patient+=1\n",
    "                X.append('-1')\n",
    "        else:\n",
    "            X.append('0')\n",
    "            Y.append('0-0-0')\n",
    "    outfile_info.write(str(new_patient)+'\\t')  \n",
    "    outfile_info.write(str(old_patient)+'\\n')    \n",
    "    for i in range(502628):\n",
    "        outfile_data.write(X[i]+'\\t')\n",
    "    outfile_data.write('\\n')\n",
    "    for i in range(502628):\n",
    "        outfile_data.write(Y[i]+'\\t')\n",
    "    outfile_data.write('\\n')\n",
    "    \n",
    "outfile_info.close()        \n",
    "outfile_data.close()                "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 10: Remove the inappropriate participants from ICD 10 codes file (disease_status).\n",
    "\n",
    "Only keep the participants in merge_white_Britich_clean.fam (generated in **Step 3**)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from operator import itemgetter\n",
    "personallid=list(np.load('/oak/stanford/groups/arend/Eric/UKBB/phenotype_13721/0.npy'))\n",
    "personusedid=np.genfromtxt('/oak/stanford/groups/jamesz/eric/genotype/merge_white_Britich_clean.fam',dtype=str)\n",
    "indexnum=0\n",
    "index_col=[]\n",
    "for i in range(337536):\n",
    "    personid=personusedid[i,0]\n",
    "    personindex=personallid.index(personid)-1\n",
    "    index_col.append(personindex)\n",
    "ICDdata=open('/oak/stanford/groups/arend/Eric/UKBB/Genenvironment/matchICD10/disease_status')\n",
    "ICDdata_new=open('/oak/stanford/groups/arend/Eric/UKBB/Genenvironment/matchICD10/disease_status_337536','w')\n",
    "linenum=0\n",
    "for line in ICDdata:\n",
    "    if linenum%2==0:\n",
    "        case_one=list(itemgetter(*index_col)(line.strip('\\n').rsplit()))\n",
    "        ICDdata_new.write(' '.join(case_one)+'\\n')\n",
    "    linenum+=1\n",
    "ICDdata.close()\n",
    "ICDdata_new.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 11: Integrate self-reported diseases and ICD 10 codes \n",
    "\n",
    "\n",
    "### 1. Define the matching table between ICD 10 codes and self-report diseases (\"icd10\", see Table S2); \n",
    "\n",
    "### 2. Merge the self-reported diseases (related_self_data) with the diseases from hospitalization records (related_icd_data).\n",
    "\n",
    "For each disease:\n",
    "\n",
    "> \\*the participants would classified as prevalent cases if they were annotated as '1' in self_cancer.data (self_disease.data) or '-1' in disease_info\n",
    "\n",
    "> \\*the participants would classified as incident cases if they were annotated as '0' in self_cancer.data (self_disease.data) and '1' in disease_info\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "phenotype=['1348','1066','1111','1134','1154','1220','1242','1265','1294','1297','1374','1473','1065','1068','1113','1136','1207','1224','1243','1293','1295','1452']\n",
    "infile1=open('self_cancer_sub.data')\n",
    "infile2=open('self_disease.list')\n",
    "outfile1=open('related_self_list','w')\n",
    "outfile2=open('related_self_data','w')\n",
    "linenum=0\n",
    "outfile1.write('1002\\n')\n",
    "for line in infile1:\n",
    "    if linenum==1:\n",
    "        outfile2.write(line)\n",
    "    linenum+=1\n",
    "alldata=np.genfromtxt('self_disease_sub.data',dtype=str)\n",
    "linenum=0\n",
    "diseaselist=[]\n",
    "for line in infile2:\n",
    "    A=line.strip('\\n').rsplit()\n",
    "    diseaselist.append(A[0])\n",
    "for index in phenotype:\n",
    "    print(linenum)\n",
    "    aa=diseaselist.index(index)\n",
    "    outfile1.write(index+'\\n')\n",
    "    outfile2.write(' '.join(alldata[aa])+'\\n')\n",
    "    linenum+=1\n",
    "outfile1.close()\n",
    "outfile2.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "phenotype=['1002','1348','1066','1111','1134','1154','1220','1242','1265','1294','1297','1374','1473','1065','1068','1113','1136','1207','1224','1243','1293','1295','1452']\n",
    "#input selfreport_icd10\n",
    "from collections import defaultdict\n",
    "selfid=defaultdict(list)\n",
    "infile=open('/oak/stanford/groups/arend/Eric/UKBB/Genenvironment/matchICD10/icd10')\n",
    "for line in infile:\n",
    "    A=line.strip('\\n').split('\\t')\n",
    "    if A[0]!='1243' and A[0]!='1242':\n",
    "        selfid[A[0]]=A[2:]\n",
    "    elif A[0]=='1243':\n",
    "        X=[]\n",
    "        for i in range(99):\n",
    "            if i<9:\n",
    "                X.append('F0'+str(i+1))\n",
    "            else:\n",
    "                X.append('F'+str(i+1))\n",
    "        selfid[A[0]]=X\n",
    "    elif A[0]=='1242':\n",
    "        X=[]\n",
    "        for i in range(60):\n",
    "            if i<10:\n",
    "                X.append('H0'+str(i))\n",
    "            else:\n",
    "                 X.append('H'+str(i))\n",
    "        selfid[A[0]]=X\n",
    "#print(selfid)\n",
    "#filterout icd10\n",
    "selfid_to_line=defaultdict(list)\n",
    "infile=open('disease_info')\n",
    "outfile1=open('related_icd_data','w')\n",
    "outfile2=open('related_icd_id','w')\n",
    "linenum=0\n",
    "for line in infile:\n",
    "    A=line.strip('\\n').rsplit()\n",
    "    for key,values in selfid.items():\n",
    "        if A[0] in values:\n",
    "            selfid_to_line[key].append(linenum)\n",
    "    linenum+=1\n",
    "icddata=np.genfromtxt('disease_status_337536',dtype=int)\n",
    "for index in phenotype:\n",
    "    values=selfid_to_line[index]\n",
    "    outfile2.write(index+'\\t')\n",
    "    linenum=0\n",
    "    sumdata=[]\n",
    "    for line in values:\n",
    "        if linenum==0:\n",
    "            sumdata=icddata[line,:]\n",
    "        else:\n",
    "            for t in range(337536):\n",
    "                if icddata[line,t]==-1:\n",
    "                    sumdata[t]=-1\n",
    "                elif icddata[line,t]==1 and sumdata[t]!=-1:\n",
    "                    sumdata[t]=1\n",
    "        linenum+=1\n",
    "    ABC=' '.join(map(str,sumdata))\n",
    "    outfile2.write(str(ABC.count('-1'))+'\\t'+str(ABC.count('1'))+'\\n')\n",
    "    XYZ=ABC.replace('1','2').replace('-2','0')\n",
    "    outfile1.write(XYZ+'\\n')\n",
    "outfile1.close()\n",
    "outfile2.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "phenotype=['1002','1348','1066','1111','1134','1154','1220','1242','1265','1294','1297','1374','1473','1065','1068','1113','1136','1207','1224','1243','1293','1295','1452']\n",
    "icddata=np.genfromtxt('related_icd_data',dtype=str)\n",
    "selfdata=np.genfromtxt('related_self_data',dtype=str)\n",
    "sex=np.genfromtxt('/oak/stanford/groups/arend/Eric/UKBB/Genenvironment/fam_disease/white_british/chip/merge_white_Britich_clean.fam',dtype=str)\n",
    "out_list=open('merge.list','w')\n",
    "out_data=open('merge.data','w')\n",
    "for m in range(23):\n",
    "    self_disease_man=0\n",
    "    self_disease_woman=0\n",
    "    self_disease_total=0\n",
    "    icd_disease_man=0\n",
    "    icd_disease_woman=0\n",
    "    icd_disease_total=0\n",
    "    control_man=0\n",
    "    control_woman=0\n",
    "    control_total=0\n",
    "    out_list.write(phenotype[m]+'\\t')\n",
    "    for n in range(337536):\n",
    "        if selfdata[m,n]=='1':\n",
    "            out_data.write('1'+' ')\n",
    "            if sex[n,4]=='1':\n",
    "                self_disease_man+=1\n",
    "            elif sex[n,4]=='2':\n",
    "                self_disease_woman+=1\n",
    "            self_disease_total+=1\n",
    "        elif selfdata[m,n]=='0' and icddata[m,n]=='0':\n",
    "            if sex[n,4]=='1':\n",
    "                control_man+=1\n",
    "            elif sex[n,4]=='2':\n",
    "                control_woman+=1\n",
    "            out_data.write('0'+' ')\n",
    "            control_total+=1\n",
    "        elif selfdata[m,n]=='0' and icddata[m,n]=='2':\n",
    "            out_data.write('2'+' ')\n",
    "            if sex[n,4]=='1':\n",
    "                icd_disease_man+=1\n",
    "            elif sex[n,4]=='2':\n",
    "                icd_disease_woman+=1\n",
    "            icd_disease_total+=1\n",
    "    out_data.write('\\n')\n",
    "    out_list.write(str(self_disease_man)+'\\t'+str(self_disease_woman)+'\\t'+str(self_disease_total)+'\\t'+str(icd_disease_man)+'\\t'+str(icd_disease_woman)+'\\t'+str(icd_disease_total)+'\\t'+str(control_man)+'\\t'+str(control_woman)+'\\t'+str(control_total)+'\\n')\n",
    "out_data.close()\n",
    "out_list.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 12: generate four fam files (prevalent cases: id_1.fam, id_2.fam, id_3.fam; incident cases: id_4.fam )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "class indinfo(object):\n",
    "    def __init__(self):\n",
    "        self.id=\"\"\n",
    "        self.gender=\"\"\n",
    "\n",
    "infile1=open('merge.list','r')\n",
    "infile2=open('merge.data','r')\n",
    "infile3=open('/oak/stanford/groups/arend/Eric/UKBB/Genenvironment/fam_disease/white_british/chip/merge_white_Britich_clean.fam','r')\n",
    "indlist=[]\n",
    "disease_num_old_man=defaultdict(list)\n",
    "disease_num_old_woman=defaultdict(list)\n",
    "disease_num_old_all=defaultdict(list)\n",
    "disease_num_new_man=defaultdict(list)\n",
    "disease_num_new_woman=defaultdict(list)\n",
    "disease_num_new_all=defaultdict(list)\n",
    "disease_num_control_man=defaultdict(list)\n",
    "disease_num_control_woman=defaultdict(list)\n",
    "disease_num_control_all=defaultdict(list)\n",
    "total_women=0\n",
    "total_man=0\n",
    "for line in infile3:\n",
    "    A=line.strip('\\n').split()\n",
    "    X=indinfo()\n",
    "    X.id=A[0]\n",
    "    #print(A[4])\n",
    "    X.gender=A[4]\n",
    "    indlist.append(X)\n",
    "phenotype=[]\n",
    "for line in infile1:\n",
    "    A=line.strip('\\n').rsplit()\n",
    "    disease_num_old_man[A[0]]=int(A[1])\n",
    "    disease_num_old_woman[A[0]]=int(A[2])\n",
    "    disease_num_old_all[A[0]]=int(A[3])\n",
    "\n",
    "    disease_num_new_man[A[0]]=int(A[4])\n",
    "    disease_num_new_woman[A[0]]=int(A[5])\n",
    "    disease_num_new_all[A[0]]=int(A[6])\n",
    "\n",
    "    disease_num_control_man[A[0]]=int(A[7])\n",
    "    disease_num_control_woman[A[0]]=int(A[8])\n",
    "    disease_num_control_all[A[0]]=int(A[9])\n",
    "    phenotype.append(A[0])\n",
    "line_index=0\n",
    "for line in infile2:\n",
    "    print(phenotype[line_index])\n",
    "    A=line.strip('\\n').split()\n",
    "    outfile1=open('/oak/stanford/groups/arend/Eric/UKBB/Genenvironment/matchICD10/'+phenotype[line_index]+'_1.fam','w')\n",
    "    outfile2=open('/oak/stanford/groups/arend/Eric/UKBB/Genenvironment/matchICD10/'+phenotype[line_index]+'_2.fam','w')\n",
    "    outfile3=open('/oak/stanford/groups/arend/Eric/UKBB/Genenvironment/matchICD10/'+phenotype[line_index]+'_3.fam','w')\n",
    "    outfile4=open('/oak/stanford/groups/arend/Eric/UKBB/Genenvironment/matchICD10/'+phenotype[line_index]+'_4.fam','w')\n",
    "    case_old_man=int(disease_num_old_man[phenotype[line_index]])\n",
    "    case_old_woman=int(disease_num_old_woman[phenotype[line_index]])\n",
    "    case_old_all=int(disease_num_old_all[phenotype[line_index]])\n",
    "    case_new_man=int(disease_num_new_man[phenotype[line_index]])\n",
    "    case_new_woman=int(disease_num_new_woman[phenotype[line_index]])\n",
    "    case_new_all=int(disease_num_new_all[phenotype[line_index]])\n",
    "    control_man=int(disease_num_control_man[phenotype[line_index]])\n",
    "    control_woman=int(disease_num_control_woman[phenotype[line_index]])\n",
    "    control_all=int(disease_num_control_all[phenotype[line_index]])\n",
    "    case_1=0\n",
    "    case_2=0\n",
    "    case_3=0\n",
    "    case_4=0\n",
    "    control_1=0\n",
    "    control_2=0\n",
    "    control_3=0\n",
    "    control_4=0\n",
    "    if phenotype[line_index]=='1002':\n",
    "        case_1=int(case_old_woman/3)\n",
    "        case_2=case_1*2\n",
    "        case_3=case_old_woman\n",
    "        case_4=case_new_woman\n",
    "        controleffect=control_woman-case_new_woman\n",
    "        control_1=int(controleffect/3)\n",
    "        control_2=control_1*2\n",
    "        control_3=controleffect\n",
    "        control_4=control_woman\n",
    "    elif phenotype[line_index]=='1348':\n",
    "        case_1=int(case_old_woman/3)\n",
    "        case_2=case_1*2\n",
    "        case_3=case_old_woman\n",
    "        case_4=case_new_woman\n",
    "        controleffect=control_woman-case_new_woman\n",
    "        #print(control_woman)\n",
    "        control_1=int(controleffect/3)\n",
    "        control_2=control_1*2\n",
    "        control_3=controleffect\n",
    "        control_4=control_woman\n",
    "    elif phenotype[line_index]=='1207':\n",
    "        case_1=int(case_old_man/3)\n",
    "        case_2=case_1*2    \n",
    "        case_3=case_old_man\n",
    "        case_4=case_new_man\n",
    "        controleffect=control_man-case_new_man\n",
    "        control_1=int(controleffect/3)\n",
    "        control_2=control_1*2\n",
    "        control_3=controleffect\n",
    "        control_4=control_man\n",
    "    else:\n",
    "        case_1=int(case_old_all/3)\n",
    "        case_2=case_1*2\n",
    "        case_3=case_old_all\n",
    "        case_4=case_new_all\n",
    "        controleffect=control_all-case_new_all\n",
    "        control_1=int(controleffect/3)\n",
    "        control_2=control_1*2\n",
    "        control_3=controleffect\n",
    "        control_4=control_all\n",
    "\n",
    "    case_num=0\n",
    "    control_num=0\n",
    "    sexlist=[]\n",
    "    if phenotype[line_index]=='1002' or phenotype[line_index]=='1348':\n",
    "        sexlist=['2']\n",
    "    elif phenotype[line_index]=='1207':\n",
    "        sexlist=['1']\n",
    "    else:\n",
    "        sexlist=['1','2']\n",
    "    for i in range(len(indlist)):\n",
    "        X=indlist[i]\n",
    "            #print(X)\n",
    "            #print(X.gender)\n",
    "        if X.gender not in sexlist:\n",
    "            outfile1.write(X.id+' '+X.id+' '+'0'+' '+'0'+' '+X.gender+' '+'-9'+'\\n')\n",
    "            outfile2.write(X.id+' '+X.id+' '+'0'+' '+'0'+' '+X.gender+' '+'-9'+'\\n')\n",
    "            outfile3.write(X.id+' '+X.id+' '+'0'+' '+'0'+' '+X.gender+' '+'-9'+'\\n')\n",
    "            outfile4.write(X.id+' '+X.id+' '+'0'+' '+'0'+' '+X.gender+' '+'-9'+'\\n')\n",
    "        if X.gender in sexlist and A[i]=='1':\n",
    "            if case_num<case_1:\n",
    "                outfile1.write(X.id+' '+X.id+' '+'0'+' '+'0'+' '+X.gender+' '+'2'+'\\n')\n",
    "                outfile2.write(X.id+' '+X.id+' '+'0'+' '+'0'+' '+X.gender+' '+'-9'+'\\n')\n",
    "                outfile3.write(X.id+' '+X.id+' '+'0'+' '+'0'+' '+X.gender+' '+'-9'+'\\n')\n",
    "                outfile4.write(X.id+' '+X.id+' '+'0'+' '+'0'+' '+X.gender+' '+'-9'+'\\n')\n",
    "            if  case_num>=case_1 and case_num<case_2:\n",
    "                outfile1.write(X.id+' '+X.id+' '+'0'+' '+'0'+' '+X.gender+' '+'-9'+'\\n')\n",
    "                outfile2.write(X.id+' '+X.id+' '+'0'+' '+'0'+' '+X.gender+' '+'2'+'\\n')\n",
    "                outfile3.write(X.id+' '+X.id+' '+'0'+' '+'0'+' '+X.gender+' '+'-9'+'\\n')\n",
    "                outfile4.write(X.id+' '+X.id+' '+'0'+' '+'0'+' '+X.gender+' '+'-9'+'\\n')\n",
    "            if case_num>=case_2:\n",
    "                outfile1.write(X.id+' '+X.id+' '+'0'+' '+'0'+' '+X.gender+' '+'-9'+'\\n')    \n",
    "                outfile2.write(X.id+' '+X.id+' '+'0'+' '+'0'+' '+X.gender+' '+'-9'+'\\n')\n",
    "                outfile3.write(X.id+' '+X.id+' '+'0'+' '+'0'+' '+X.gender+' '+'2'+'\\n')\n",
    "                outfile4.write(X.id+' '+X.id+' '+'0'+' '+'0'+' '+X.gender+' '+'-9'+'\\n')\n",
    "            case_num+=1\n",
    "        elif X.gender in sexlist and A[i]=='2':\n",
    "            outfile1.write(X.id+' '+X.id+' '+'0'+' '+'0'+' '+X.gender+' '+'-9'+'\\n')\n",
    "            outfile2.write(X.id+' '+X.id+' '+'0'+' '+'0'+' '+X.gender+' '+'-9'+'\\n')\n",
    "            outfile3.write(X.id+' '+X.id+' '+'0'+' '+'0'+' '+X.gender+' '+'-9'+'\\n')\n",
    "            outfile4.write(X.id+' '+X.id+' '+'0'+' '+'0'+' '+X.gender+' '+'2'+'\\n')\n",
    "        elif X.gender in sexlist and A[i]=='0':\n",
    "            if control_num<=control_1:\n",
    "                outfile1.write(X.id+' '+X.id+' '+'0'+' '+'0'+' '+X.gender+' '+'1'+'\\n')\n",
    "                outfile2.write(X.id+' '+X.id+' '+'0'+' '+'0'+' '+X.gender+' '+'-9'+'\\n')\n",
    "                outfile3.write(X.id+' '+X.id+' '+'0'+' '+'0'+' '+X.gender+' '+'-9'+'\\n')\n",
    "                outfile4.write(X.id+' '+X.id+' '+'0'+' '+'0'+' '+X.gender+' '+'-9'+'\\n')\n",
    "            elif control_num>control_1 and control_num<=control_2:\n",
    "                outfile1.write(X.id+' '+X.id+' '+'0'+' '+'0'+' '+X.gender+' '+'-9'+'\\n')\n",
    "                outfile2.write(X.id+' '+X.id+' '+'0'+' '+'0'+' '+X.gender+' '+'1'+'\\n')\n",
    "                outfile3.write(X.id+' '+X.id+' '+'0'+' '+'0'+' '+X.gender+' '+\"-9\"+'\\n')\n",
    "                outfile4.write(X.id+' '+X.id+' '+'0'+' '+'0'+' '+X.gender+' '+'-9'+'\\n')\n",
    "            elif control_num>control_2 and control_num<=control_3:\n",
    "                outfile1.write(X.id+' '+X.id+' '+'0'+' '+'0'+' '+X.gender+' '+\"-9\"+'\\n')\n",
    "                outfile2.write(X.id+' '+X.id+' '+'0'+' '+'0'+' '+X.gender+' '+\"-9\"+'\\n')\n",
    "                outfile3.write(X.id+' '+X.id+' '+'0'+' '+'0'+' '+X.gender+' '+'1'+'\\n')\n",
    "                outfile4.write(X.id+' '+X.id+' '+'0'+' '+'0'+' '+X.gender+' '+'-9'+'\\n')\n",
    "            elif control_num>control_3 and control_num<=control_4:\n",
    "                outfile1.write(X.id+' '+X.id+' '+'0'+' '+'0'+' '+X.gender+' '+\"-9\"+'\\n')\n",
    "                outfile2.write(X.id+' '+X.id+' '+'0'+' '+'0'+' '+X.gender+' '+\"-9\"+'\\n')\n",
    "                outfile3.write(X.id+' '+X.id+' '+'0'+' '+'0'+' '+X.gender+' '+'-9'+'\\n')\n",
    "                outfile4.write(X.id+' '+X.id+' '+'0'+' '+'0'+' '+X.gender+' '+'1'+'\\n')\n",
    "            control_num+=1\n",
    "    outfile1.close()\n",
    "    outfile2.close()\n",
    "    outfile3.close()\n",
    "    outfile4.close()\n",
    "    line_index+=1\n",
    "infile1.close()\n",
    "infile2.close()\n",
    "infile3.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Step 13: Generate control pool "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "patient=defaultdict(str)\n",
    "phenotype=['1002','1348','1066','1111','1134','1154','1220','1242','1265','1294','1297','1374','1065','1068','1113','1136','1207','1224','1243','1293','1295','1452']\n",
    "for disease in phenotype:\n",
    "    print(disease)\n",
    "    for index in ['1','2','3','4']:\n",
    "        infile=open(disease+'_'+index+'.fam')\n",
    "        for line in infile:\n",
    "            A=line.rsplit()\n",
    "            if A[5]=='2':\n",
    "                patient[A[0]]='0'\n",
    "infile1=open('merge_white_Britich_clean.fam','r')\n",
    "out1=open('health_control','w')\n",
    "out2=open('health_control_male','w')\n",
    "out3=open('health_control_female','w')\n",
    "\n",
    "all_control=0\n",
    "male_control=0\n",
    "female_control=0\n",
    "\n",
    "for line in infile1:\n",
    "    A=line.rsplit()\n",
    "    if A[0] not in patient.keys():\n",
    "        out1.write(A[0]+'\\n')\n",
    "        all_control+=1\n",
    "        if A[4]=='1':\n",
    "            out2.write(A[0]+'\\n')\n",
    "            male_control+=1\n",
    "        if A[4]=='2':\n",
    "            out3.write(A[0]+'\\n')\n",
    "            female_control+=1\n",
    "infile1.close()\n",
    "print(all_control)\n",
    "print(male_control)\n",
    "print(female_control)\n",
    "out1.close()\n",
    "out2.close()\n",
    "out3.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 14: Generate training, validation and test sets (id_train.fam,id_valid.fam,id_test.fam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import random\n",
    "import multiprocessing\n",
    "infile1=open('merge_white_Britich_clean.fam','r')\n",
    "infile2=open('health_control','r')\n",
    "infile3=open('health_control_male','r')\n",
    "infile4=open('health_control_female','r')\n",
    "\n",
    "all_fam=[]\n",
    "indid=[]\n",
    "for line in infile1:\n",
    "    A=line.rsplit()\n",
    "    all_fam.append(A[0]+' '+A[1]+' '+A[2]+' '+A[3]+' '+A[4]+' ')\n",
    "    indid.append(A[0])\n",
    "controllist=[]\n",
    "control_male=[]\n",
    "control_female=[]\n",
    "for line in infile2:\n",
    "    controllist.append(line.strip('\\n'))\n",
    "for line in infile3:\n",
    "    control_male.append(line.strip('\\n'))\n",
    "for line in infile4:\n",
    "    control_female.append(line.strip('\\n'))\n",
    "infile1.close()\n",
    "infile2.close()\n",
    "infile3.close()\n",
    "infile4.close()\n",
    "phenotype=['1002','1348','1066','1111','1134','1154','1220','1242','1265','1294','1297','1374','1473','1065','1068','1113','1136','1207','1224','1243','1293','1295','1452']\n",
    "def generate(disease):\n",
    "    print(disease)\n",
    "    out_train=open(disease+'_train.fam','w')\n",
    "    out_valid=open(disease+'_valid.fam','w')\n",
    "    out_test=open(disease+'_test.fam','w')\n",
    "    case_train=[]\n",
    "    control_train=[]\n",
    "    case_valid=[]\n",
    "    control_valid=[]\n",
    "    case_test=[]\n",
    "    control_test=[]\n",
    "\n",
    "    for index in ['1','2']:\n",
    "        infile=open(disease+'_'+index+'.fam')\n",
    "        for line in infile:\n",
    "            A=line.strip('\\n').rsplit()\n",
    "            if A[5]=='2':\n",
    "                case_train.append(A[0])\n",
    "        infile.close()\n",
    "\n",
    "    infile=open(disease+'_'+'3.fam')\n",
    "    for line in infile:\n",
    "        A=line.strip('\\n').rsplit()\n",
    "        if A[5]=='2':\n",
    "            case_valid.append(A[0])\n",
    "    infile.close()\n",
    "\n",
    "    infile=open(disease+'_'+'4.fam')\n",
    "    for line in infile:\n",
    "        A=line.strip('\\n').rsplit()\n",
    "        if A[5]=='2':\n",
    "            case_test.append(A[0])\n",
    "    infile.close()\n",
    "\n",
    "    if disease=='1002' or disease=='1348':\n",
    "        if len(case_train)>=len(control_female):\n",
    "            control_train=control_female\n",
    "        else:\n",
    "            control_train=random.sample(control_female,len(case_train))\n",
    "        if len(case_valid)>=len(control_female):\n",
    "            control_valid=control_female\n",
    "        else:\n",
    "            control_valid=random.sample(control_female,len(case_valid))\n",
    "        if len(case_test)>=len(control_female):\n",
    "            control_test=control_female\n",
    "        else:\n",
    "            control_test=random.sample(control_female,len(case_test))\n",
    "    elif disease=='1207':\n",
    "        if len(case_train)>=len(control_male):\n",
    "            control_train=control_male\n",
    "        else:\n",
    "            control_train=random.sample(control_male,len(case_train))\n",
    "        if len(case_valid)>=len(control_male):\n",
    "            control_valid=control_male\n",
    "        else:\n",
    "            control_valid=random.sample(control_male,len(case_valid))\n",
    "        if len(case_test)>=len(control_male):\n",
    "            control_test=control_male\n",
    "        else:\n",
    "            control_test=random.sample(control_male,len(case_test))\n",
    "    else:                \n",
    "        if len(case_train)>=len(controllist):\n",
    "            control_train=controllist\n",
    "        else:\n",
    "            control_train=random.sample(controllist,len(case_train))\n",
    "\n",
    "        if len(case_valid)>=len(controllist):\n",
    "            control_valid=controllist\n",
    "        else:\n",
    "            control_valid=random.sample(controllist,len(case_valid))\n",
    "        if len(case_test)>=len(controllist):\n",
    "            control_test=controllist\n",
    "        else:\n",
    "            control_test=random.sample(controllist,len(case_test))\n",
    "\n",
    "    for x in range(len(all_fam)):\n",
    "        label='-9'\n",
    "        if indid[x] in case_train:\n",
    "            label='2'\n",
    "        elif indid[x] in control_train:\n",
    "            label='1'\n",
    "        out_train.write(all_fam[x]+label+'\\n')\n",
    "        label='-9'\n",
    "        if indid[x] in case_valid:\n",
    "            label='2'\n",
    "        elif indid[x] in control_valid:\n",
    "            label='1'\n",
    "        out_valid.write(all_fam[x]+label+'\\n')\n",
    "        label='-9'\n",
    "        if indid[x] in case_test:\n",
    "            label='2'\n",
    "        elif indid[x] in control_test:\n",
    "            label='1'\n",
    "        out_test.write(all_fam[x]+label+'\\n')\n",
    "    out_train.close()\n",
    "    out_valid.close()\n",
    "    out_test.close()\n",
    "for disease in phenotype:\n",
    "    generate(disease)                      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 15: Perform genome wide association study for 22 diseases\n",
    "\"covar_full.list\" includes age, gender and top 10 principal components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import multiprocessing\n",
    "phenotype=['1002','1348','1066','1111','1134','1154','1220','1242','1265','1294','1297','1374','1065','1068','1113','1136','1207','1224','1243','1293','1295','1452']\n",
    "for x in phenotype:\n",
    "    print(x)\n",
    "    out=open(x+'_assoc.sh','w')\n",
    "    out.write('cp '+x+'_train.fam merge_white_Britich_clean_'+x+'.fam'+'\\n')\n",
    "    if x in ['1002','1348','1207']:\n",
    "        out.write(\"plink --bfile merge_white_Britich_clean_\"+x+\" --geno 0.1 --hwe 0.000001 --maf 0.01 --mind 0.1 --adjust --logistic --covar covar_full.list --covar-number 1,3-12 --ci 0.95 --out \"+str(x)+\"_assoc_train_adjust\\n\")\n",
    "    else:\n",
    "        out.write(\"plink --bfile merge_white_Britich_clean_\"+x+\" --geno 0.1 --hwe 0.000001 --maf 0.01 --mind 0.1 --adjust --logistic --covar covar_full.list --ci 0.95 --out \"+str(x)+\"_assoc_train_adjust\\n\")\n",
    "\n",
    "    out.write('cp '+x+'_valid.fam merge_white_Britich_clean_'+x+'.fam'+'\\n')\n",
    "    if x in ['1002','1348','1207']:\n",
    "        out.write(\"plink --bfile merge_white_Britich_clean_\"+x+\" --geno 0.1 --hwe 0.000001 --maf 0.01 --mind 0.1 --adjust --logistic --covar covar_full.list --covar-number 1,3-12 --ci 0.95 --out \"+str(x)+\"_assoc_valid_adjust\\n\")\n",
    "    else:\n",
    "        out.write(\"plink --bfile merge_white_Britich_clean_\"+x+\" --geno 0.1 --hwe 0.000001 --maf 0.01 --mind 0.1 --adjust --logistic --covar covar_full.list --ci 0.95 --out \"+str(x)+\"_assoc_valid_adjust\\n\")\n",
    "\n",
    "\n",
    "    out.write('cp '+x+'_test.fam merge_white_Britich_clean_'+x+'.fam'+'\\n')\n",
    "    if x in ['1002','1348','1207']:\n",
    "        out.write(\"plink --bfile merge_white_Britich_clean_\"+x+\" --geno 0.1 --hwe 0.000001 --maf 0.01 --mind 0.1 --adjust --logistic --covar covar_full.list --covar-number 1,3-12 --ci 0.95 --out \"+str(x)+\"_assoc_test_adjust\\n\")\n",
    "    else:\n",
    "        out.write(\"plink --bfile merge_white_Britich_clean_\"+x+\" --geno 0.1 --hwe 0.000001 --maf 0.01 --mind 0.1 --adjust --logistic --covar covar_full.list --ci 0.95 --out \"+str(x)+\"_assoc_test_adjust\\n\") \n",
    "    out.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 16: The pipeline of calculate polygenic risk score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate SNP list  with p-values < 5E-3, < 5E-4, < 5E-5, < 5E-6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "phenotype=['1002','1348','1066','1111','1134','1154','1220','1242','1265','1294','1297','1374','1065','1068','1113','1136','1207','1224','1243','1293','1295','1452']\n",
    "def generate(index):\n",
    "    print(index)\n",
    "    valid=defaultdict(float)\n",
    "    test=defaultdict(float)\n",
    "    infile1=open('../'+index+'_assoc_train_adjust.assoc.logistic.adjusted')\n",
    "    snplist1=open('./SNPselection/'+index+'_5E3_SNP','w')\n",
    "    snplist2=open('./SNPselection/'+index+'_5E4_SNP','w')\n",
    "    snplist3=open('./SNPselection/'+index+'_5E5_SNP','w')\n",
    "    snplist4=open('./SNPselection/'+index+'_5E6_SNP','w')\n",
    "\n",
    "    SNP1=0\n",
    "    SNP2=0\n",
    "    SNP3=0\n",
    "    SNP4=0\n",
    "\n",
    "    linenum=0\n",
    "    for line in infile1:\n",
    "        if linenum>0:\n",
    "            A=line.strip('\\n').rsplit()\n",
    "            if float(A[3])<0.005:\n",
    "                snplist1.write(A[1]+'\\n')\n",
    "                SNP1+=1\n",
    "            if float(A[3])<0.0005:\n",
    "                snplist2.write(A[1]+'\\n')\n",
    "                SNP2+=1\n",
    "            if float(A[3])<0.00005:\n",
    "                snplist3.write(A[1]+'\\n')\n",
    "                SNP3+=1\n",
    "            if float(A[3])<0.000005:\n",
    "                snplist4.write(A[1]+'\\n')\n",
    "                SNP4+=1\n",
    "        linenum+=1\n",
    "    snpvalidate.write(str(index)+' '+str(SNP1)+' '+str(SNP2)+' '+str(SNP3)+' '+str(SNP4)+'\\n')\n",
    "    infile1.close()\n",
    "    snplist1.close()\n",
    "    snplist2.close()\n",
    "    snplist3.close()\n",
    "    snplist4.close()\n",
    "snpvalidate=open('./SNPselection/stat','w')\n",
    "for index in phenotype:\n",
    "    print(index)\n",
    "    generate(index)\n",
    "snpvalidate.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract the SNP genotypes from SNP list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "phenotype=['1002','1348','1066','1111','1134','1154','1220','1242','1265','1294','1297','1374','1065','1068','1113','1136','1207','1224','1243','1293','1295','1452']\n",
    "def generate(index):\n",
    "    print(index)\n",
    "    os.system('cp ../'+index+'_train.fam ../merge_white_Britich_clean_'+index+'.fam')\n",
    "    os.system('plink --bfile ../merge_white_Britich_clean_'+index+' --extract ./SNPselection/'+index+'_5E6_SNP --make-bed --out ./SNPextraction/'+index+'_5E6_train_extract'+'\\n')\n",
    "    os.system('plink --bfile ../merge_white_Britich_clean_'+index+' --extract ./SNPselection/'+index+'_5E5_SNP --make-bed --out ./SNPextraction/'+index+'_5E5_train_extract'+'\\n')\n",
    "    os.system('plink --bfile ../merge_white_Britich_clean_'+index+' --extract ./SNPselection/'+index+'_5E4_SNP --make-bed --out ./SNPextraction/'+index+'_5E4_train_extract'+'\\n')\n",
    "    os.system('plink --bfile ../merge_white_Britich_clean_'+index+' --extract ./SNPselection/'+index+'_5E3_SNP --make-bed --out ./SNPextraction/'+index+'_5E3_train_extract'+'\\n')\n",
    "\n",
    "    os.system('cp ../'+index+'_valid.fam ../merge_white_Britich_clean_'+index+'.fam')\n",
    "    os.system('plink --bfile ../merge_white_Britich_clean_'+index+' --extract ./SNPselection/'+index+'_5E6_SNP --make-bed --out ./SNPextraction/'+index+'_5E6_valid_extract'+'\\n')\n",
    "    os.system('plink --bfile ../merge_white_Britich_clean_'+index+' --extract ./SNPselection/'+index+'_5E5_SNP --make-bed --out ./SNPextraction/'+index+'_5E5_valid_extract'+'\\n')\n",
    "    os.system('plink --bfile ../merge_white_Britich_clean_'+index+' --extract ./SNPselection/'+index+'_5E4_SNP --make-bed --out ./SNPextraction/'+index+'_5E4_valid_extract'+'\\n')\n",
    "    os.system('plink --bfile ../merge_white_Britich_clean_'+index+' --extract ./SNPselection/'+index+'_5E3_SNP --make-bed --out ./SNPextraction/'+index+'_5E3_valid_extract'+'\\n')\n",
    "\n",
    "    os.system('cp ../'+index+'_test.fam ../merge_white_Britich_clean_'+index+'.fam')\n",
    "    os.system('plink --bfile ../merge_white_Britich_clean_'+index+' --extract ./SNPselection/'+index+'_5E6_SNP --make-bed --out ./SNPextraction/'+index+'_5E6_test_extract'+'\\n')\n",
    "    os.system('plink --bfile ../merge_white_Britich_clean_'+index+' --extract ./SNPselection/'+index+'_5E5_SNP --make-bed --out ./SNPextraction/'+index+'_5E5_test_extract'+'\\n')\n",
    "    os.system('plink --bfile ../merge_white_Britich_clean_'+index+' --extract ./SNPselection/'+index+'_5E4_SNP --make-bed --out ./SNPextraction/'+index+'_5E4_test_extract'+'\\n')\n",
    "    os.system('plink --bfile ../merge_white_Britich_clean_'+index+' --extract ./SNPselection/'+index+'_5E3_SNP --make-bed --out ./SNPextraction/'+index+'_5E3_test_extract'+'\\n')\n",
    "for index in phenotype:\n",
    "    generate(index)    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate linkage disequilibrium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "phenotype=['1002','1348','1066','1111','1134','1154','1220','1242','1265','1294','1297','1374','1065','1068','1113','1136','1207','1224','1243','1293','1295','1452']\n",
    "def generate(index):\n",
    "    print(index)\n",
    "    os.system('plink --bfile ./SNPextraction/'+index+'_5E6_train_extract --indep-pairwise 50 5 0.5 --out ./LDpruning/train_5E6_'+index+'\\n')\n",
    "    os.system('plink --bfile ./SNPextraction/'+index+'_5E5_train_extract --indep-pairwise 50 5 0.5 --out ./LDpruning/train_5E5_'+index+'\\n')\n",
    "    os.system('plink --bfile ./SNPextraction/'+index+'_5E4_train_extract --indep-pairwise 50 5 0.5 --out ./LDpruning/train_5E4_'+index+'\\n')\n",
    "    os.system('plink --bfile ./SNPextraction/'+index+'_5E3_train_extract --indep-pairwise 50 5 0.5 --out ./LDpruning/train_5E3_'+index+'\\n')\n",
    "\n",
    "    os.system('plink --bfile ./SNPextraction/'+index+'_5E6_valid_extract --indep-pairwise 50 5 0.5 --out ./LDpruning/valid_'+index+'\\n')\n",
    "    os.system('plink --bfile ./SNPextraction/'+index+'_5E5_valid_extract --indep-pairwise 50 5 0.5 --out ./LDpruning/valid_'+index+'\\n')\n",
    "    os.system('plink --bfile ./SNPextraction/'+index+'_5E4_valid_extract --indep-pairwise 50 5 0.5 --out ./LDpruning/valid_'+index+'\\n')\n",
    "    os.system('plink --bfile ./SNPextraction/'+index+'_5E3_train_extract --indep-pairwise 50 5 0.5 --out ./LDpruning/valid_'+index+'\\n')\n",
    "\n",
    "    os.system('plink --bfile ./SNPextraction/'+index+'_5E6_test_extract --indep-pairwise 50 5 0.5 --out ./LDpruning/test_'+index+'\\n')\n",
    "    os.system('plink --bfile ./SNPextraction/'+index+'_5E5_test_extract --indep-pairwise 50 5 0.5 --out ./LDpruning/test_'+index+'\\n')\n",
    "    os.system('plink --bfile ./SNPextraction/'+index+'_5E4_test_extract --indep-pairwise 50 5 0.5 --out ./LDpruning/test_'+index+'\\n')\n",
    "    os.system('plink --bfile ./SNPextraction/'+index+'_5E3_test_extract --indep-pairwise 50 5 0.5 --out ./LDpruning/test_'+index+'\\n')\n",
    "for index in phenotype:\n",
    "    generate(index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract independent SNPs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import multiprocessing\n",
    "phenotype=['1002','1348','1066','1111','1134','1154','1220','1242','1265','1294','1297','1374','1065','1068','1113','1136','1207','1224','1243','1293','1295','1452']\n",
    "def generate(index):\n",
    "    print(index)\n",
    "    os.system('plink --bfile ./SNPextraction/'+index+'_5E6_train_extract --extract ./LDpruning/train_5E6_'+index+'.prune.in --make-bed --out ./final/'+index+'_train_5E6_SNP')\n",
    "    os.system('plink --bfile ./SNPextraction/'+index+'_5E5_train_extract --extract ./LDpruning/train_5E5_'+index+'.prune.in --make-bed --out ./final/'+index+'_train_5E5_SNP')\n",
    "    os.system('plink --bfile ./SNPextraction/'+index+'_5E4_train_extract --extract ./LDpruning/train_5E4_'+index+'.prune.in --make-bed --out ./final/'+index+'_train_5E4_SNP')\n",
    "    os.system('plink --bfile ./SNPextraction/'+index+'_5E3_train_extract --extract ./LDpruning/train_5E3_'+index+'.prune.in --make-bed --out ./final/'+index+'_train_5E3_SNP')\n",
    "\n",
    "    os.system('plink --bfile ./final/'+index+'_train_5E6_SNP --recodeA --out ./final/'+index+'_train_5E6_recodeA')\n",
    "    os.system('plink --bfile ./final/'+index+'_train_5E5_SNP --recodeA --out ./final/'+index+'_train_5E5_recodeA')\n",
    "    os.system('plink --bfile ./final/'+index+'_train_5E4_SNP --recodeA --out ./final/'+index+'_train_5E4_recodeA')\n",
    "    os.system('plink --bfile ./final/'+index+'_train_5E3_SNP --recodeA --out ./final/'+index+'_train_5E3_recodeA')\n",
    "\n",
    "\n",
    "    os.system('plink --bfile ./SNPextraction/'+index+'_5E6_valid_extract --extract ./LDpruning/train_5E6_'+index+'.prune.in --make-bed --out ./final/'+index+'_valid_5E6_SNP')\n",
    "    os.system('plink --bfile ./SNPextraction/'+index+'_5E5_valid_extract --extract ./LDpruning/train_5E5_'+index+'.prune.in --make-bed --out ./final/'+index+'_valid_5E5_SNP')\n",
    "    os.system('plink --bfile ./SNPextraction/'+index+'_5E4_valid_extract --extract ./LDpruning/train_5E4_'+index+'.prune.in --make-bed --out ./final/'+index+'_valid_5E4_SNP')\n",
    "    os.system('plink --bfile ./SNPextraction/'+index+'_5E3_valid_extract --extract ./LDpruning/train_5E3_'+index+'.prune.in --make-bed --out ./final/'+index+'_valid_5E3_SNP')\n",
    "\n",
    "    os.system('plink --bfile ./final/'+index+'_valid_5E6_SNP --recodeA --out ./final/'+index+'_valid_5E6_recodeA')\n",
    "    os.system('plink --bfile ./final/'+index+'_valid_5E5_SNP --recodeA --out ./final/'+index+'_valid_5E5_recodeA')\n",
    "    os.system('plink --bfile ./final/'+index+'_valid_5E4_SNP --recodeA --out ./final/'+index+'_valid_5E4_recodeA')\n",
    "    os.system('plink --bfile ./final/'+index+'_valid_5E3_SNP --recodeA --out ./final/'+index+'_valid_5E3_recodeA')\n",
    "\n",
    "\n",
    "    os.system('plink --bfile ./SNPextraction/'+index+'_5E6_test_extract --extract ./LDpruning/train_5E6_'+index+'.prune.in --make-bed --out ./final/'+index+'_test_5E6_SNP')\n",
    "    os.system('plink --bfile ./SNPextraction/'+index+'_5E5_test_extract --extract ./LDpruning/train_5E5_'+index+'.prune.in --make-bed --out ./final/'+index+'_test_5E5_SNP')\n",
    "    os.system('plink --bfile ./SNPextraction/'+index+'_5E4_test_extract --extract ./LDpruning/train_5E4_'+index+'.prune.in --make-bed --out ./final/'+index+'_test_5E4_SNP')\n",
    "    os.system('plink --bfile ./SNPextraction/'+index+'_5E3_test_extract --extract ./LDpruning/train_5E3_'+index+'.prune.in --make-bed --out ./final/'+index+'_test_5E3_SNP')\n",
    "\n",
    "    os.system('plink --bfile ./final/'+index+'_test_5E6_SNP --recodeA --out ./final/'+index+'_test_5E6_recodeA')\n",
    "    os.system('plink --bfile ./final/'+index+'_test_5E5_SNP --recodeA --out ./final/'+index+'_test_5E5_recodeA')\n",
    "    os.system('plink --bfile ./final/'+index+'_test_5E4_SNP --recodeA --out ./final/'+index+'_test_5E4_recodeA')\n",
    "    os.system('plink --bfile ./final/'+index+'_test_5E3_SNP --recodeA --out ./final/'+index+'_test_5E3_recodeA')\n",
    "\n",
    "\n",
    "for index in phenotype:\n",
    "    generate(index)    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculated polygenic risk score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from sklearn.metrics import roc_auc_score\n",
    "phenotype=['1002','1348','1066','1111','1134','1154','1220','1242','1265','1294','1297','1374','1065','1068','1113','1136','1207','1224','1243','1293','1295','1452']\n",
    "def cal_GRS(infile,EF):\n",
    "    linenum=0\n",
    "    total_snp=0\n",
    "    diseaseid=[]\n",
    "    effectsize_0=[]\n",
    "    effectsize_1=[]\n",
    "    effectsize_2=[]\n",
    "    GRSlist=[]\n",
    "    for line in infile:\n",
    "        A=line.strip('\\n').split(' ')\n",
    "        GRS=0\n",
    "        if linenum==0:\n",
    "            total_snp=len(A)-6\n",
    "            for x in range(total_snp):\n",
    "                B=A[x+6].split('_')\n",
    "                C=EF[B[0]].split('_')\n",
    "                if B[1]==C[0]:\n",
    "                    effectsize_0.append(0)\n",
    "                    effectsize_1.append(float(C[1]))\n",
    "                    effectsize_2.append(2*float(C[1]))\n",
    "                else:\n",
    "                    effectsize_2.append(0)\n",
    "                    effectsize_1.append(float(C[1]))\n",
    "                    effectsize_0.append(2*float(C[1]))\n",
    "        else:\n",
    "            if A[5]!='-9':\n",
    "                if A[5]=='2':\n",
    "                    diseaseid.append(1)\n",
    "                elif A[5]=='1':\n",
    "                    diseaseid.append(0)\n",
    "                for x in range(total_snp):\n",
    "                    if A[x+6]=='0':\n",
    "                        GRS=GRS+effectsize_0[x]\n",
    "                    elif A[x+6]=='1':\n",
    "                        GRS=GRS+effectsize_1[x]\n",
    "                    elif A[x+6]=='2':\n",
    "                        GRS=GRS+effectsize_2[x]\n",
    "                GRSlist.append(GRS)\n",
    "        linenum+=1\n",
    "    return GRSlist,diseaseid\n",
    "\n",
    "\n",
    "def generate(index):\n",
    "    EF=defaultdict(str)\n",
    "    GRS_valid_5E3=[]\n",
    "    GRS_valid_5E4=[]\n",
    "    GRS_valid_5E5=[]\n",
    "    GRS_valid_5E6=[]\n",
    "    disease_valid_5E3=[]\n",
    "    disease_valid_5E4=[]\n",
    "    disease_valid_5E5=[]\n",
    "    disease_valid_5E6=[]\n",
    "\n",
    "    infile1=open('../'+index+'_assoc_train_adjust.assoc.logistic')\n",
    "    infile2=open('./final/'+index+'_valid_5E3_recodeA.raw')\n",
    "    infile3=open('./final/'+index+'_valid_5E4_recodeA.raw')\n",
    "    infile4=open('./final/'+index+'_valid_5E5_recodeA.raw')\n",
    "    infile5=open('./final/'+index+'_valid_5E6_recodeA.raw')\n",
    "\n",
    "    linenum=0\n",
    "    for line in infile1:\n",
    "        A=line.strip('\\n').rsplit()\n",
    "        if linenum>0 and A[4]=='ADD':\n",
    "            EF[A[1]]=A[3]+'_'+A[10]\n",
    "        linenum+=1\n",
    "\n",
    "    GRS_valid_5E3,disease_valid_5E3=cal_GRS(infile2,EF)\n",
    "    GRS_valid_5E4,disease_valid_5E4=cal_GRS(infile3,EF)\n",
    "    GRS_valid_5E5,disease_valid_5E5=cal_GRS(infile4,EF)\n",
    "    GRS_valid_5E6,disease_valid_5E6=cal_GRS(infile5,EF)\n",
    "\n",
    "    score1=roc_auc_score(disease_valid_5E3,GRS_valid_5E3)\n",
    "    score2=roc_auc_score(disease_valid_5E4,GRS_valid_5E4)\n",
    "    score3=roc_auc_score(disease_valid_5E5,GRS_valid_5E5)\n",
    "    score4=roc_auc_score(disease_valid_5E6,GRS_valid_5E6)\n",
    "    score=[]\n",
    "    score.append(score1)\n",
    "    score.append(score2)\n",
    "    score.append(score3)\n",
    "    score.append(score4)\n",
    "    maxscore=max(score)\n",
    "    indexmax=score.index(maxscore)\n",
    "    scoretest=0\n",
    "    if indexmax==0:\n",
    "        infile=open('./final/'+index+'_test_5E3_recodeA.raw')\n",
    "        GRS_test,disease_test=cal_GRS(infile,EF)\n",
    "        scoretest=roc_auc_score(disease_test,GRS_test)\n",
    "        infile.close()\n",
    "    elif indexmax==1:\n",
    "        infile=open('./final/'+index+'_test_5E4_recodeA.raw')\n",
    "        GRS_test,disease_test=cal_GRS(infile,EF)\n",
    "        scoretest=roc_auc_score(disease_test,GRS_test)\n",
    "        infile.close()\n",
    "    elif indexmax==2:\n",
    "        infile=open('./final/'+index+'_test_5E5_recodeA.raw')\n",
    "        GRS_test,disease_test=cal_GRS(infile,EF)\n",
    "        scoretest=roc_auc_score(disease_test,GRS_test)\n",
    "        infile.close()\n",
    "    elif indexmax==3:\n",
    "        infile=open('./final/'+index+'_test_5E6_recodeA.raw')\n",
    "        GRS_test,disease_test=cal_GRS(infile,EF)\n",
    "        scoretest=roc_auc_score(disease_test,GRS_test)\n",
    "        infile.close()\n",
    "    infile1.close()\n",
    "    infile2.close()\n",
    "    infile3.close()\n",
    "    infile4.close()\n",
    "    infile5.close()\n",
    "    return scoretest\n",
    "\n",
    "result=open('./GRS_result','w')\n",
    "for index in phenotype:\n",
    "    scoretest=generate(index)\n",
    "    result.write(index+'\\t'+str(scoretest)+'\\n')\n",
    "result.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 17: Reform genotype data, L&E, covariate  information into .npy matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import matplotlib\n",
    "import re\n",
    "import numpy as np\n",
    "import operator\n",
    "from scipy import stats as sta\n",
    "import math\n",
    "import multiprocessing\n",
    "import operator\n",
    "import scipy\n",
    "from itertools import groupby\n",
    "from operator import itemgetter \n",
    "def generate_G_P(path_phenotype,path_data):\n",
    "    phenotype=np.genfromtxt(path_phenotype,dtype=str) \n",
    "    data=np.genfromtxt(path_data,dtype=str)\n",
    "    SNPindexlist=[]\n",
    "    SNP_id=[]\n",
    "    ind_id=[]\n",
    "    indindexlist=[]\n",
    "    ind_genotype=[]\n",
    "    all_disease=[]\n",
    "    totalind=phenotype.shape[0]\n",
    "    print(totalind)\n",
    "    for i in range(totalind):\n",
    "        print(i)\n",
    "        if i==0:\n",
    "            snpnum=data[0].shape[0]\n",
    "            for j in range(6,snpnum):\n",
    "                SNPindexlist.append(j)\n",
    "                SNP_id.append(data[0][j].split('_')[0])\n",
    "        else:\n",
    "                if len(SNPindexlist)==1:\n",
    "                    tmp=data[i][6]\n",
    "                    if tmp=='NA':\n",
    "                        tmp='0'\n",
    "                else:\n",
    "                    tmp=list(itemgetter(*SNPindexlist)(data[i]))\n",
    "                    for x in range(len(tmp)):\n",
    "                        if tmp[x]=='NA':\n",
    "                            tmp[x]='0'\n",
    "                indindexlist.append(i-1)\n",
    "                ind_genotype.append(list(map(int,tmp)))   \n",
    "                ind_id.append(phenotype[i-1][0])\n",
    "                all_disease.append(int(data[i][5])-1)\n",
    "    all_genotype=sta.zscore(ind_genotype)\n",
    "    return all_genotype,all_disease,ind_id,SNP_id\n",
    "\n",
    "\n",
    "def Cal_score(index):\n",
    "    #print(index)\n",
    "    outpath1='/oak/stanford/groups/arend/Eric/UKBB/Genenvironment/meta_analysis/data_extraction/train/'\n",
    "    outpath2='/oak/stanford/groups/arend/Eric/UKBB/Genenvironment/meta_analysis/data_extraction/valid/'\n",
    "    outpath3='/oak/stanford/groups/arend/Eric/UKBB/Genenvironment/meta_analysis/data_extraction/test/'\n",
    "\n",
    "    out1_disease=outpath1+str(index)+'_disease_train'\n",
    "    out2_disease=outpath2+str(index)+'_disease_valid'\n",
    "    out3_disease=outpath3+str(index)+'_disease_test'\n",
    "     \n",
    "    out1_covar=outpath1+str(index)+'_covar_train'\n",
    "    out2_covar=outpath2+str(index)+'_covar_valid'\n",
    "    out3_covar=outpath3+str(index)+'_covar_test'\n",
    "    \n",
    "    out1_geno_train=outpath1+str(index)+'_genotype_train_5E3'\n",
    "    out2_geno_train=outpath1+str(index)+'_genotype_train_5E4'\n",
    "    out3_geno_train=outpath1+str(index)+'_genotype_train_5E5'\n",
    "    out4_geno_train=outpath1+str(index)+'_genotype_train_5E6'\n",
    "\n",
    "    out1_geno_valid=outpath2+str(index)+'_genotype_valid_5E3'\n",
    "    out2_geno_valid=outpath2+str(index)+'_genotype_valid_5E4'\n",
    "    out3_geno_valid=outpath2+str(index)+'_genotype_valid_5E5'\n",
    "    out4_geno_valid=outpath2+str(index)+'_genotype_valid_5E6'\n",
    "\n",
    "    out1_geno_test=outpath3+str(index)+'_genotype_test_5E3'\n",
    "    out2_geno_test=outpath3+str(index)+'_genotype_test_5E4'\n",
    "    out3_geno_test=outpath3+str(index)+'_genotype_test_5E5'\n",
    "    out4_geno_test=outpath3+str(index)+'_genotype_test_5E6'\n",
    "\n",
    "    out_snpid_5E3=outpath1+str(index)+'_snpid_5E3'\n",
    "    out_snpid_5E4=outpath1+str(index)+'_snpid_5E4'\n",
    "    out_snpid_5E5=outpath1+str(index)+'_snpid_5E5'\n",
    "    out_snpid_5E6=outpath1+str(index)+'_snpid_5E6'\n",
    "\n",
    "\n",
    "    out1_id=outpath1+str(index)+'_indid_train'\n",
    "    out2_id=outpath2+str(index)+'_indid_valid'\n",
    "    out3_id=outpath3+str(index)+'_indid_test'\n",
    "\n",
    "    [train_genotype_1,train_disease_1,train_indid_1,train_SNPid_1]=generate_G_P(dir_path+index+'_train_5E3_SNP.fam',dir_path+index+'_train_5E3_recodeA.raw')\n",
    "    [valid_genotype_1,valid_disease_1,valid_indid_1,valid_SNPid_1]=generate_G_P(dir_path+index+'_valid_5E3_SNP.fam',dir_path+index+'_valid_5E3_recodeA.raw')\n",
    "    [test_genotype_1,test_disease_1,test_indid_1,test_SNPid_1]=generate_G_P(dir_path+index+'_test_5E3_SNP.fam',dir_path+index+'_test_5E3_recodeA.raw')\n",
    "\n",
    "    np.save(out_snpid_5E3,train_SNPid_1)\n",
    "    np.save(out1_geno_train,train_genotype_1)\n",
    "    np.save(out1_geno_valid,valid_genotype_1)\n",
    "    np.save(out1_geno_test,test_genotype_1)\n",
    "    np.save(out1_id,train_indid_1)\n",
    "    np.save(out2_id,valid_indid_1)\n",
    "    np.save(out3_id,test_indid_1)\n",
    "    np.save(out1_disease,train_disease_1)\n",
    "    np.save(out2_disease,valid_disease_1)\n",
    "    np.save(out3_disease,test_disease_1)\n",
    "\n",
    "    train_genotype_1=[]\n",
    "    train_disease_1=[]\n",
    "    train_indid_1=[]\n",
    "    train_SNPid_1=[]\n",
    "    valid_genotype_1=[]\n",
    "    valid_disease_1=[]\n",
    "    valid_indid_1=[]\n",
    "    valid_SNPid_1=[]\n",
    "    test_genotype_1=[]\n",
    "    test_disease_1=[]\n",
    "    test_indid_1=[]\n",
    "    test_SNPid_1=[]\n",
    "\n",
    "    [train_genotype_2,train_disease_2,train_indid_2,train_SNPid_2]=generate_G_P(dir_path+index+'_train_5E4_SNP.fam',dir_path+index+'_train_5E4_recodeA.raw')\n",
    "    [valid_genotype_2,valid_disease_2,valid_indid_2,valid_SNPid_2]=generate_G_P(dir_path+index+'_valid_5E4_SNP.fam',dir_path+index+'_valid_5E4_recodeA.raw')\n",
    "    [test_genotype_2,test_disease_2,test_indid_2,test_SNPid_2]=generate_G_P(dir_path+index+'_test_5E4_SNP.fam',dir_path+index+'_test_5E4_recodeA.raw')\n",
    " \n",
    "    np.save(out_snpid_5E4,train_SNPid_2)\n",
    "    np.save(out2_geno_train,train_genotype_2)\n",
    "    np.save(out2_geno_valid,valid_genotype_2)\n",
    "    np.save(out2_geno_test,test_genotype_2)\n",
    "\n",
    "    train_genotype_2=[]\n",
    "    train_disease_2=[]\n",
    "    train_indid_2=[]\n",
    "    train_SNPid_2=[]\n",
    "    valid_genotype_2=[]\n",
    "    valid_disease_2=[]\n",
    "    valid_indid_2=[]\n",
    "    valid_SNPid_2=[]\n",
    "    test_genotype_2=[]\n",
    "    test_disease_2=[]\n",
    "    test_indid_2=[]\n",
    "    test_SNPid_2=[]\n",
    "    \n",
    "    [train_genotype_3,train_disease_3,train_indid_3,train_SNPid_3]=generate_G_P(dir_path+index+'_train_5E5_SNP.fam',dir_path+index+'_train_5E5_recodeA.raw')\n",
    "    [valid_genotype_3,valid_disease_3,valid_indid_3,valid_SNPid_3]=generate_G_P(dir_path+index+'_valid_5E5_SNP.fam',dir_path+index+'_valid_5E5_recodeA.raw')\n",
    "    [test_genotype_3,test_disease_3,test_indid_3,test_SNPid_3]=generate_G_P(dir_path+index+'_test_5E5_SNP.fam',dir_path+index+'_test_5E5_recodeA.raw')\n",
    "    \n",
    "\n",
    "    np.save(out_snpid_5E5,train_SNPid_3)\n",
    "    np.save(out3_geno_train,train_genotype_3)\n",
    "    np.save(out3_geno_valid,valid_genotype_3)\n",
    "    np.save(out3_geno_test,test_genotype_3)\n",
    "\n",
    "    train_genotype_3=[]\n",
    "    train_disease_3=[]\n",
    "    train_indid_3=[]\n",
    "    train_SNPid_3=[]\n",
    "    valid_genotype_3=[]\n",
    "    valid_disease_3=[]\n",
    "    valid_indid_3=[]\n",
    "    valid_SNPid_3=[]\n",
    "    test_genotype_3=[]\n",
    "    test_disease_3=[]\n",
    "    test_indid_3=[]\n",
    "    test_SNPid_3=[]\n",
    "\n",
    "    [train_genotype_4,train_disease_4,train_indid_4,train_SNPid_4]=generate_G_P(dir_path+index+'_train_5E6_SNP.fam',dir_path+index+'_train_5E6_recodeA.raw')\n",
    "    [valid_genotype_4,valid_disease_4,valid_indid_4,valid_SNPid_4]=generate_G_P(dir_path+index+'_valid_5E6_SNP.fam',dir_path+index+'_valid_5E6_recodeA.raw')\n",
    "    [test_genotype_4,test_disease_4,test_indid_4,test_SNPid_4]=generate_G_P(dir_path+index+'_test_5E6_SNP.fam',dir_path+index+'_test_5E6_recodeA.raw')\n",
    "\n",
    "\n",
    "    np.save(out_snpid_5E6,train_SNPid_4)\n",
    "    np.save(out4_geno_train,train_genotype_4)\n",
    "    np.save(out4_geno_valid,valid_genotype_4)\n",
    "    np.save(out4_geno_test,test_genotype_4)\n",
    "\n",
    "    train_genotype_4=[]\n",
    "    train_disease_4=[]\n",
    "    train_indid_4=[]\n",
    "    train_SNPid_4=[]\n",
    "    valid_genotype_4=[]\n",
    "    valid_disease_4=[]\n",
    "    valid_indid_4=[]\n",
    "    valid_SNPid_4=[]\n",
    "    test_genotype_4=[]\n",
    "    test_disease_4=[]\n",
    "    test_indid_4=[]\n",
    "    test_SNPid_4=[]\n",
    "\n",
    "    num1=len(train_indid_1)\n",
    "    num2=len(valid_indid_1)\n",
    "    num3=len(test_indid_1)\n",
    "\n",
    "    cov1=[]\n",
    "    cov2=[]\n",
    "    cov3=[]\n",
    "\n",
    "    for i in range(num1):\n",
    "        if len(cov1)==0:\n",
    "            cov1=covlist[indall.index(train_indid_1[i]),2:]\n",
    "        else:\n",
    "            cov1=np.vstack((cov1,covlist[indall.index(train_indid_1[i]),2:]))\n",
    "    for i in range(num2):\n",
    "        if len(cov2)==0:\n",
    "            cov2=covlist[indall.index(valid_indid_1[i]),2:]\n",
    "        else:\n",
    "            cov2=np.vstack((cov2,covlist[indall.index(valid_indid_1[i]),2:]))\n",
    "    for i in range(num3):\n",
    "        if len(cov3)==0:\n",
    "            cov3=covlist[indall.index(test_indid_1[i]),2:]\n",
    "        else:\n",
    "            cov3=np.vstack((cov3,covlist[indall.index(test_indid_1[i]),2:]))\n",
    "    np.save(out1_covar, cov1)\n",
    "    np.save(out2_covar, cov2)\n",
    "    np.save(out3_covar, cov3)\n",
    "\n",
    "covlist=np.loadtxt('covar_full.list',dtype=float)\n",
    "indall=[]\n",
    "infile1=open('/oak/stanford/groups/arend/Eric/UKBB/Genenvironment/fam_disease/white_british/chip/merge_white_Britich_clean_train.fam')\n",
    "for line in infile1:\n",
    "    A=line.strip('\\n').split()\n",
    "    indall.append(A[0])\n",
    "infile1.close()\n",
    "\n",
    "phenotype=['threshold']\n",
    "dir_path='/oak/stanford/groups/arend/Eric/UKBB/Genenvironment/fam_disease/white_british/chip/threshold_hypertension/final/'\n",
    "for index in phenotype:\n",
    "    print(index)\n",
    "    Cal_score(index)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 18: Remove the effects of age and gender from L&E features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import matplotlib\n",
    "import re\n",
    "import numpy as np\n",
    "import operator\n",
    "from scipy import stats as sta\n",
    "import math\n",
    "from scipy.interpolate import spline\n",
    "import operator\n",
    "from itertools import groupby\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from operator import itemgetter\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from scipy.stats.stats import pearsonr\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn import linear_model\n",
    "from scipy.stats import norm\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import multiprocessing\n",
    "import pickle\n",
    "\n",
    "dir_path1='/oak/stanford/groups/arend/Eric/UKBB/Genenvironment/meta_analysis/data_extraction/train/'\n",
    "dir_path2='/oak/stanford/groups/arend/Eric/UKBB/Genenvironment/meta_analysis/data_extraction/valid/'\n",
    "dir_path3='/oak/stanford/groups/arend/Eric/UKBB/Genenvironment/meta_analysis/data_extraction/test/'\n",
    "#phenotype=['1002','1348','1066','1111','1134','1154','1220','1242','1265','1294','1297','1374','1065','1068','1113','1136','1207','1224','1243','1293','1295','1452']\n",
    "phenotype=['1242']\n",
    "for index in phenotype: \n",
    "    print(index)\n",
    "    Pheno_train=np.load(dir_path1+index+'_phenotype_train.npy')\n",
    "    Pheno_valid=np.load(dir_path2+index+'_phenotype_valid.npy')\n",
    "    Pheno_test=np.load(dir_path3+index+'_phenotype_test.npy')\n",
    "\n",
    "    Cov_train=np.load(dir_path1+index+'_covar_train.npy')\n",
    "    Cov_valid=np.load(dir_path2+index+'_covar_valid.npy')\n",
    "    Cov_test=np.load(dir_path3+index+'_covar_test.npy')\n",
    "\n",
    "    Matrix_residual_train=[]\n",
    "    Matrix_residual_valid=[]\n",
    "    Matrix_residual_test=[]\n",
    "\n",
    "    for i in range(88):\n",
    "        print(i)\n",
    "        A_train=Pheno_train[:,i]\n",
    "        B_train=Cov_train[:,0:2]\n",
    "        reg_train=LinearRegression().fit(B_train,A_train)\n",
    "        residual_train=np.expand_dims(reg_train.predict(B_train)-A_train,axis=1)\n",
    "\n",
    "        A_valid=Pheno_valid[:,i]\n",
    "        B_valid=Cov_valid[:,0:2]\n",
    "        reg_valid=LinearRegression().fit(B_valid,A_valid)\n",
    "        residual_valid=np.expand_dims(reg_valid.predict(B_valid)-A_valid,axis=1)\n",
    "\n",
    "        A_test=Pheno_test[:,i]\n",
    "        B_test=Cov_test[:,0:2]\n",
    "        reg_test=LinearRegression().fit(B_test,A_test)\n",
    "        residual_test=np.expand_dims(reg_test.predict(B_test)-A_test,axis=1)\n",
    "\n",
    "        if i==0:\n",
    "            Matrix_residual_train=residual_train\n",
    "            Matrix_residual_valid=residual_valid\n",
    "            Matrix_residual_test=residual_test\n",
    "        else:           \n",
    "            Matrix_residual_train=np.concatenate((Matrix_residual_train,residual_train),axis=1)\n",
    "            Matrix_residual_valid=np.concatenate((Matrix_residual_valid,residual_valid),axis=1)\n",
    "            Matrix_residual_test=np.concatenate((Matrix_residual_test,residual_test),axis=1)\n",
    "\n",
    "    np.save('./train/'+index+'_train_residual.npy',Matrix_residual_train)\n",
    "    np.save('./valid/'+index+'_valid_residual.npy',Matrix_residual_valid)\n",
    "    np.save('./test/'+index+'_test_residual.npy',Matrix_residual_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Step 19: Disease prediction by genotype only"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lasso regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import matplotlib\n",
    "import re\n",
    "import numpy as np\n",
    "import operator\n",
    "from scipy import stats as sta\n",
    "import math\n",
    "from scipy.interpolate import spline\n",
    "import operator\n",
    "from itertools import groupby\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from operator import itemgetter\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from scipy.stats.stats import pearsonr\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn import linear_model\n",
    "from scipy.stats import norm\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import multiprocessing\n",
    "import pickle\n",
    "\n",
    "dir_path1='/oak/stanford/groups/arend/Eric/UKBB/Genenvironment/meta_analysis/data_extraction/train/'\n",
    "dir_path2='/oak/stanford/groups/arend/Eric/UKBB/Genenvironment/meta_analysis/data_extraction/valid/'\n",
    "dir_path3='/oak/stanford/groups/arend/Eric/UKBB/Genenvironment/meta_analysis/data_extraction/test/'\n",
    "#phenotype=['1002','1348','1066','1111','1134','1154','1220','1242','1265','1294','1297','1374','1065','1068','1113','1136','1207','1224','1243','1293','1295','1452']\n",
    "phenotype=['1242']\n",
    "def predict(index):\n",
    "    print('loading train')\n",
    "    Geno_train_5E3=np.load(dir_path1+index+'_genotype_train_5E3.npy')\n",
    "    Geno_train_5E4=np.load(dir_path1+index+'_genotype_train_5E4.npy')\n",
    "    Geno_train_5E5=np.load(dir_path1+index+'_genotype_train_5E5.npy')\n",
    "    Geno_train_5E6=np.load(dir_path1+index+'_genotype_train_5E6.npy')\n",
    "    print('loading train')\n",
    "\n",
    "    print('loading valid')\n",
    "    Geno_valid_5E3=np.load(dir_path2+index+'_genotype_valid_5E3.npy')\n",
    "    Geno_valid_5E4=np.load(dir_path2+index+'_genotype_valid_5E4.npy')\n",
    "    Geno_valid_5E5=np.load(dir_path2+index+'_genotype_valid_5E5.npy')\n",
    "    Geno_valid_5E6=np.load(dir_path2+index+'_genotype_valid_5E6.npy')\n",
    "    print('loading valid')\n",
    "\n",
    "    print('loading test')\n",
    "    Geno_test_5E3=np.load(dir_path3+index+'_genotype_test_5E3.npy')\n",
    "    Geno_test_5E4=np.load(dir_path3+index+'_genotype_test_5E4.npy')\n",
    "    Geno_test_5E5=np.load(dir_path3+index+'_genotype_test_5E5.npy')\n",
    "    Geno_test_5E6=np.load(dir_path3+index+'_genotype_test_5E6.npy')\n",
    "    print('loading test')\n",
    "\n",
    "    disease_train=np.load(dir_path1+index+'_disease_train.npy')\n",
    "    disease_valid=np.load(dir_path2+index+'_disease_valid.npy')\n",
    "    disease_test=np.load(dir_path3+index+'_disease_test.npy')\n",
    "\n",
    "    AUC_max=0\n",
    "    Cx_max=0\n",
    "    s_best=0\n",
    "\n",
    "    for s in [1,2,3,4]:\n",
    "        print(s)\n",
    "        for Cx in [0.0001,0.001,0.01,0.1]:\n",
    "            LR1 = LogisticRegression(penalty='l1', C=Cx, max_iter=10000)\n",
    "            AUC_valid=0\n",
    "            if s==1:\n",
    "                LR1.fit(Geno_train_5E3,disease_train)\n",
    "                Y1=LR1.predict_proba(Geno_valid_5E3)\n",
    "                AUC_valid=roc_auc_score(disease_valid,Y1[:,1])\n",
    "            elif s==2:\n",
    "                LR1.fit(Geno_train_5E4,disease_train)\n",
    "                Y1=LR1.predict_proba(Geno_valid_5E4)\n",
    "                AUC_valid=roc_auc_score(disease_valid,Y1[:,1])\n",
    "            elif s==3:\n",
    "                LR1.fit(Geno_train_5E5,disease_train)\n",
    "                Y1=LR1.predict_proba(Geno_valid_5E5)\n",
    "                AUC_valid=roc_auc_score(disease_valid,Y1[:,1])\n",
    "            elif s==4:\n",
    "                LR1.fit(Geno_train_5E6,disease_train)\n",
    "                Y1=LR1.predict_proba(Geno_valid_5E6)\n",
    "                AUC_valid=roc_auc_score(disease_valid,Y1[:,1])\n",
    "            if AUC_valid>AUC_max:\n",
    "                AUC_max=AUC_valid\n",
    "                Cx_max=Cx\n",
    "                s_best=s\n",
    "            \n",
    "    LR2 = LogisticRegression(penalty='l1', C=Cx_max, max_iter=10000)\n",
    "    AUC_test=0\n",
    "    if s_best==1:\n",
    "        LR2.fit(Geno_train_5E3,disease_train)\n",
    "        Y1=LR2.predict_proba(Geno_valid_5E3)\n",
    "        Y2=LR2.predict_proba(Geno_test_5E3)\n",
    "        AUC_test=roc_auc_score(disease_test,Y2[:,1])\n",
    "        np.save('./LR/'+index+'_valid.npy',Y1)\n",
    "        np.save('./LR/'+index+'_test.npy',Y2)\n",
    "    elif s_best==2:\n",
    "        LR2.fit(Geno_train_5E4,disease_train)\n",
    "        Y1=LR2.predict_proba(Geno_valid_5E4)\n",
    "        Y2=LR2.predict_proba(Geno_test_5E4)\n",
    "        AUC_test=roc_auc_score(disease_test,Y2[:,1])\n",
    "        np.save('./LR/'+index+'_valid.npy',Y1)\n",
    "        np.save('./LR/'+index+'_test.npy',Y2)\n",
    "    elif s_best==3:\n",
    "        LR2.fit(Geno_train_5E5,disease_train)\n",
    "        Y1=LR2.predict_proba(Geno_valid_5E5)\n",
    "        Y2=LR2.predict_proba(Geno_test_5E5)\n",
    "        AUC_test=roc_auc_score(disease_test,Y2[:,1])\n",
    "        np.save('./LR/'+index+'_valid.npy',Y1)\n",
    "        np.save('./LR/'+index+'_test.npy',Y2)\n",
    "    elif s_best==4:\n",
    "        LR2.fit(Geno_train_5E6,disease_train)\n",
    "        Y1=LR2.predict_proba(Geno_valid_5E6)\n",
    "        Y2=LR2.predict_proba(Geno_test_5E6)\n",
    "        AUC_test=roc_auc_score(disease_test,Y2[:,1])\n",
    "        np.save('./LR/'+index+'_valid.npy',Y1)\n",
    "        np.save('./LR/'+index+'_test.npy',Y2)\n",
    "    out=open('./LR/LR_'+index,'w')\n",
    "    out.write(index+'\\t'+str(AUC_max)+'\\t'+str(AUC_test)+'\\n')\n",
    "    out.close() \n",
    "for m in phenotype:\n",
    "    predict(m)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import matplotlib\n",
    "import re\n",
    "import numpy as np\n",
    "import operator\n",
    "from scipy import stats as sta\n",
    "import math\n",
    "from scipy.interpolate import spline\n",
    "import operator\n",
    "from itertools import groupby\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from operator import itemgetter\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from scipy.stats.stats import pearsonr\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn import linear_model\n",
    "from scipy.stats import norm\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import multiprocessing\n",
    "import pickle\n",
    "from  sklearn.neural_network import MLPClassifier\n",
    "dir_path1='/oak/stanford/groups/arend/Eric/UKBB/Genenvironment/meta_analysis/data_extraction/train/'\n",
    "dir_path2='/oak/stanford/groups/arend/Eric/UKBB/Genenvironment/meta_analysis/data_extraction/valid/'\n",
    "dir_path3='/oak/stanford/groups/arend/Eric/UKBB/Genenvironment/meta_analysis/data_extraction/test/'\n",
    "phenotype=['1002','1348','1066','1111','1134','1154','1220','1242','1265','1294','1297','1374','1065','1068','1113','1136','1207','1224','1243','1293','1295','1452']\n",
    "def predict(index):\n",
    "    print('loading train')\n",
    "    Geno_train_5E3=np.load(dir_path1+index+'_genotype_train_5E3.npy')\n",
    "    Geno_train_5E4=np.load(dir_path1+index+'_genotype_train_5E4.npy')\n",
    "    Geno_train_5E5=np.load(dir_path1+index+'_genotype_train_5E5.npy')\n",
    "    Geno_train_5E6=np.load(dir_path1+index+'_genotype_train_5E6.npy')\n",
    "    print('loading train')\n",
    "\n",
    "    print('loading valid')\n",
    "    Geno_valid_5E3=np.load(dir_path2+index+'_genotype_valid_5E3.npy')\n",
    "    Geno_valid_5E4=np.load(dir_path2+index+'_genotype_valid_5E4.npy')\n",
    "    Geno_valid_5E5=np.load(dir_path2+index+'_genotype_valid_5E5.npy')\n",
    "    Geno_valid_5E6=np.load(dir_path2+index+'_genotype_valid_5E6.npy')\n",
    "    print('loading valid')\n",
    "\n",
    "    print('loading test')\n",
    "    Geno_test_5E3=np.load(dir_path3+index+'_genotype_test_5E3.npy')\n",
    "    Geno_test_5E4=np.load(dir_path3+index+'_genotype_test_5E4.npy')\n",
    "    Geno_test_5E5=np.load(dir_path3+index+'_genotype_test_5E5.npy')\n",
    "    Geno_test_5E6=np.load(dir_path3+index+'_genotype_test_5E6.npy')\n",
    "    print('loading test')\n",
    "\n",
    "    disease_train=np.load(dir_path1+index+'_disease_train.npy')\n",
    "    disease_valid=np.load(dir_path2+index+'_disease_valid.npy')\n",
    "    disease_test=np.load(dir_path3+index+'_disease_test.npy')\n",
    "\n",
    "    AUC_max=0\n",
    "    Cx_max=0\n",
    "    s_best=0\n",
    "\n",
    "    for s in [1,2,3,4]:\n",
    "        print(s)\n",
    "        for Cx in [(20,20,20),(30,30),(10,10,10,10),(30,20,10)]:\n",
    "            LR1=MLPClassifier(hidden_layer_sizes=Cx,max_iter=1000)\n",
    "            AUC_valid=0\n",
    "            if s==1:\n",
    "                LR1.fit(Geno_train_5E3,disease_train)\n",
    "                Y1=LR1.predict_proba(Geno_valid_5E3)\n",
    "                AUC_valid=roc_auc_score(disease_valid,Y1[:,1])\n",
    "            elif s==2:\n",
    "                LR1.fit(Geno_train_5E4,disease_train)\n",
    "                Y1=LR1.predict_proba(Geno_valid_5E4)\n",
    "                AUC_valid=roc_auc_score(disease_valid,Y1[:,1])\n",
    "            elif s==3:\n",
    "                LR1.fit(Geno_train_5E5,disease_train)\n",
    "                Y1=LR1.predict_proba(Geno_valid_5E5)\n",
    "                AUC_valid=roc_auc_score(disease_valid,Y1[:,1])\n",
    "            elif s==4:\n",
    "                LR1.fit(Geno_train_5E6,disease_train)\n",
    "                Y1=LR1.predict_proba(Geno_valid_5E6)\n",
    "                AUC_valid=roc_auc_score(disease_valid,Y1[:,1])\n",
    "            if AUC_valid>AUC_max:\n",
    "                AUC_max=AUC_valid\n",
    "                Cx_max=Cx\n",
    "                s_best=s\n",
    "    LR2=MLPClassifier(hidden_layer_sizes=Cx_max,max_iter=1000)\n",
    "    AUC_test=0\n",
    "    if s_best==1:\n",
    "        LR2.fit(Geno_train_5E3,disease_train)\n",
    "        Y1=LR2.predict_proba(Geno_valid_5E3)\n",
    "        Y2=LR2.predict_proba(Geno_test_5E3)\n",
    "        AUC_test=roc_auc_score(disease_test,Y2[:,1])\n",
    "        np.save('./NN/'+index+'_valid.npy',Y1)\n",
    "        np.save('./NN/'+index+'_test.npy',Y2)\n",
    "    elif s_best==2:\n",
    "        LR2.fit(Geno_train_5E4,disease_train)\n",
    "        Y1=LR2.predict_proba(Geno_valid_5E4)\n",
    "        Y2=LR2.predict_proba(Geno_test_5E4)\n",
    "        AUC_test=roc_auc_score(disease_test,Y2[:,1])\n",
    "        np.save('./NN/'+index+'_valid.npy',Y1)\n",
    "        np.save('./NN/'+index+'_test.npy',Y2)\n",
    "    elif s_best==3:\n",
    "        LR2.fit(Geno_train_5E5,disease_train)\n",
    "        Y1=LR2.predict_proba(Geno_valid_5E5)\n",
    "        Y2=LR2.predict_proba(Geno_test_5E5)\n",
    "        AUC_test=roc_auc_score(disease_test,Y2[:,1])\n",
    "        np.save('./NN/'+index+'_valid.npy',Y1)\n",
    "        np.save('./NN/'+index+'_test.npy',Y2)\n",
    "    elif s_best==4:\n",
    "        LR2.fit(Geno_train_5E6,disease_train)\n",
    "        Y1=LR2.predict_proba(Geno_valid_5E6)\n",
    "        Y2=LR2.predict_proba(Geno_test_5E6)\n",
    "        AUC_test=roc_auc_score(disease_test,Y2[:,1])\n",
    "        np.save('./NN/'+index+'_valid.npy',Y1)\n",
    "        np.save('./NN/'+index+'_test.npy',Y2)\n",
    "    out=open('./NN/NN_'+index,'w')\n",
    "    outstructure=open('./NN/select_structure_'+index,'w')\n",
    "    out.write(index+'\\t'+str(AUC_max)+'\\t'+str(AUC_test)+'\\n')\n",
    "    outstructure.write(index+'\\t'+str(s_best)+'\\t'+str(Cx_max)+'\\n')\n",
    "    out.close() \n",
    "    outstructure.close()\n",
    "for m in phenotype:\n",
    "    predict(m)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import matplotlib\n",
    "import re\n",
    "import numpy as np\n",
    "import operator\n",
    "from scipy import stats as sta\n",
    "import math\n",
    "from scipy.interpolate import spline\n",
    "import operator\n",
    "from itertools import groupby\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from operator import itemgetter\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from scipy.stats.stats import pearsonr\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn import linear_model\n",
    "from scipy.stats import norm\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import multiprocessing\n",
    "import pickle\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "dir_path1='/oak/stanford/groups/arend/Eric/UKBB/Genenvironment/meta_analysis/data_extraction/train/'\n",
    "dir_path2='/oak/stanford/groups/arend/Eric/UKBB/Genenvironment/meta_analysis/data_extraction/valid/'\n",
    "dir_path3='/oak/stanford/groups/arend/Eric/UKBB/Genenvironment/meta_analysis/data_extraction/test/'\n",
    "phenotype=['1002','1348','1066','1111','1134','1154','1220','1242','1265','1294','1297','1374','1065','1068','1113','1136','1207','1224','1243','1293','1295','1452']\n",
    "def predict(index):\n",
    "    print('loading train')\n",
    "    Geno_train_5E3=np.load(dir_path1+index+'_genotype_train_5E3.npy')\n",
    "    Geno_train_5E4=np.load(dir_path1+index+'_genotype_train_5E4.npy')\n",
    "    Geno_train_5E5=np.load(dir_path1+index+'_genotype_train_5E5.npy')\n",
    "    Geno_train_5E6=np.load(dir_path1+index+'_genotype_train_5E6.npy')\n",
    "    print('loading train')\n",
    "\n",
    "    print('loading valid')\n",
    "    Geno_valid_5E3=np.load(dir_path2+index+'_genotype_valid_5E3.npy')\n",
    "    Geno_valid_5E4=np.load(dir_path2+index+'_genotype_valid_5E4.npy')\n",
    "    Geno_valid_5E5=np.load(dir_path2+index+'_genotype_valid_5E5.npy')\n",
    "    Geno_valid_5E6=np.load(dir_path2+index+'_genotype_valid_5E6.npy')\n",
    "    print('loading valid')\n",
    "\n",
    "    print('loading test')\n",
    "    Geno_test_5E3=np.load(dir_path3+index+'_genotype_test_5E3.npy')\n",
    "    Geno_test_5E4=np.load(dir_path3+index+'_genotype_test_5E4.npy')\n",
    "    Geno_test_5E5=np.load(dir_path3+index+'_genotype_test_5E5.npy')\n",
    "    Geno_test_5E6=np.load(dir_path3+index+'_genotype_test_5E6.npy')\n",
    "    print('loading test')\n",
    "\n",
    "    disease_train=np.load(dir_path1+index+'_disease_train.npy')\n",
    "    disease_valid=np.load(dir_path2+index+'_disease_valid.npy')\n",
    "    disease_test=np.load(dir_path3+index+'_disease_test.npy')\n",
    "\n",
    "\n",
    "    AUC_max=0\n",
    "    Cx_max=0\n",
    "    s_best=0\n",
    "\n",
    "    for s in [1,2,3,4]:\n",
    "        print(s)\n",
    "        for Cx in [0.0001,0.001,0.01,0.1]:\n",
    "            LR1 = RandomForestClassifier(min_impurity_decrease=Cx)\n",
    "            AUC_valid=0\n",
    "            if s==1:\n",
    "                LR1.fit(Geno_train_5E3,disease_train)\n",
    "                Y1=LR1.predict_proba(Geno_valid_5E3)\n",
    "                AUC_valid=roc_auc_score(disease_valid,Y1[:,1])\n",
    "            elif s==2:\n",
    "                LR1.fit(Geno_train_5E4,disease_train)\n",
    "                Y1=LR1.predict_proba(Geno_valid_5E4)\n",
    "                AUC_valid=roc_auc_score(disease_valid,Y1[:,1])\n",
    "            elif s==3:\n",
    "                LR1.fit(Geno_train_5E5,disease_train)\n",
    "                Y1=LR1.predict_proba(Geno_valid_5E5)\n",
    "                AUC_valid=roc_auc_score(disease_valid,Y1[:,1])\n",
    "            elif s==4:\n",
    "                LR1.fit(Geno_train_5E6,disease_train)\n",
    "                Y1=LR1.predict_proba(Geno_valid_5E6)\n",
    "                AUC_valid=roc_auc_score(disease_valid,Y1[:,1])\n",
    "            if AUC_valid>AUC_max:\n",
    "                AUC_max=AUC_valid\n",
    "                Cx_max=Cx\n",
    "                s_best=s\n",
    "            \n",
    "    LR2 = RandomForestClassifier(min_impurity_decrease=Cx_max)\n",
    "    AUC_test=0\n",
    "    if s_best==1:\n",
    "        LR2.fit(Geno_train_5E3,disease_train)\n",
    "        Y1=LR2.predict_proba(Geno_valid_5E3)\n",
    "        Y2=LR2.predict_proba(Geno_test_5E3)\n",
    "        AUC_test=roc_auc_score(disease_test,Y2[:,1])\n",
    "        np.save('./RF/'+index+'_valid.npy',Y1)\n",
    "        np.save('./RF/'+index+'_test.npy',Y2)\n",
    "    elif s_best==2:\n",
    "        LR2.fit(Geno_train_5E4,disease_train)\n",
    "        Y1=LR2.predict_proba(Geno_valid_5E4)\n",
    "        Y2=LR2.predict_proba(Geno_test_5E4)\n",
    "        AUC_test=roc_auc_score(disease_test,Y2[:,1])\n",
    "        np.save('./RF/'+index+'_valid.npy',Y1)\n",
    "        np.save('./RF/'+index+'_test.npy',Y2)\n",
    "    elif s_best==3:\n",
    "        LR2.fit(Geno_train_5E5,disease_train)\n",
    "        Y1=LR2.predict_proba(Geno_valid_5E5)\n",
    "        Y2=LR2.predict_proba(Geno_test_5E5)\n",
    "        AUC_test=roc_auc_score(disease_test,Y2[:,1])\n",
    "        np.save('./RF/'+index+'_valid.npy',Y1)\n",
    "        np.save('./RF/'+index+'_test.npy',Y2)\n",
    "    elif s_best==4:\n",
    "        LR2.fit(Geno_train_5E6,disease_train)\n",
    "        Y1=LR2.predict_proba(Geno_valid_5E6)\n",
    "        Y2=LR2.predict_proba(Geno_test_5E6)\n",
    "        AUC_test=roc_auc_score(disease_test,Y2[:,1])\n",
    "        np.save('./RF/'+index+'_valid.npy',Y1)\n",
    "        np.save('./RF/'+index+'_test.npy',Y2)\n",
    "    out=open('./RF/RF_'+index,'w')\n",
    "    out.write(index+'\\t'+str(AUC_max)+'\\t'+str(AUC_test)+'\\n')\n",
    "    out.close() \n",
    "for m in phenotype:\n",
    "    predict(m)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adaboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import matplotlib\n",
    "import re\n",
    "import numpy as np\n",
    "import operator\n",
    "from scipy import stats as sta\n",
    "import math\n",
    "from scipy.interpolate import spline\n",
    "import operator\n",
    "from itertools import groupby\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from operator import itemgetter\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from scipy.stats.stats import pearsonr\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn import linear_model\n",
    "from scipy.stats import norm\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import multiprocessing\n",
    "import pickle\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.tree import ExtraTreeClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "dir_path1='/oak/stanford/groups/arend/Eric/UKBB/Genenvironment/meta_analysis/data_extraction/train/'\n",
    "dir_path2='/oak/stanford/groups/arend/Eric/UKBB/Genenvironment/meta_analysis/data_extraction/valid/'\n",
    "dir_path3='/oak/stanford/groups/arend/Eric/UKBB/Genenvironment/meta_analysis/data_extraction/test/'\n",
    "phenotype=['1002','1348','1066','1111','1134','1154','1220','1242','1265','1294','1297','1374','1065','1068','1113','1136','1207','1224','1243','1293','1295','1452']\n",
    "def predict(index):\n",
    "    print('loading train')\n",
    "    Geno_train_5E3=np.load(dir_path1+index+'_genotype_train_5E3.npy')\n",
    "    Geno_train_5E4=np.load(dir_path1+index+'_genotype_train_5E4.npy')\n",
    "    Geno_train_5E5=np.load(dir_path1+index+'_genotype_train_5E5.npy')\n",
    "    Geno_train_5E6=np.load(dir_path1+index+'_genotype_train_5E6.npy')\n",
    "    print('loading train')\n",
    "\n",
    "    print('loading valid')\n",
    "    Geno_valid_5E3=np.load(dir_path2+index+'_genotype_valid_5E3.npy')\n",
    "    Geno_valid_5E4=np.load(dir_path2+index+'_genotype_valid_5E4.npy')\n",
    "    Geno_valid_5E5=np.load(dir_path2+index+'_genotype_valid_5E5.npy')\n",
    "    Geno_valid_5E6=np.load(dir_path2+index+'_genotype_valid_5E6.npy')\n",
    "    print('loading valid')\n",
    "\n",
    "    print('loading test')\n",
    "    Geno_test_5E3=np.load(dir_path3+index+'_genotype_test_5E3.npy')\n",
    "    Geno_test_5E4=np.load(dir_path3+index+'_genotype_test_5E4.npy')\n",
    "    Geno_test_5E5=np.load(dir_path3+index+'_genotype_test_5E5.npy')\n",
    "    Geno_test_5E6=np.load(dir_path3+index+'_genotype_test_5E6.npy')\n",
    "    print('loading test')\n",
    "\n",
    "    disease_train=np.load(dir_path1+index+'_disease_train.npy')\n",
    "    disease_valid=np.load(dir_path2+index+'_disease_valid.npy')\n",
    "    disease_test=np.load(dir_path3+index+'_disease_test.npy')\n",
    "\n",
    "    AUC_max=0\n",
    "    Cx_max=''\n",
    "    s_best=0\n",
    "\n",
    "    for s in [1,2,3,4]:\n",
    "        print(s)\n",
    "        for Cx in [DecisionTreeClassifier(),LogisticRegression(),ExtraTreeClassifier(),GaussianNB()]:\n",
    "            print(Cx)\n",
    "            LR1 = AdaBoostClassifier(base_estimator=Cx)\n",
    "            AUC_valid=0\n",
    "            if s==1:\n",
    "                LR1.fit(Geno_train_5E3,disease_train)\n",
    "                Y1=LR1.predict_proba(Geno_valid_5E3)\n",
    "                AUC_valid=roc_auc_score(disease_valid,Y1[:,1])\n",
    "            elif s==2:\n",
    "                LR1.fit(Geno_train_5E4,disease_train)\n",
    "                Y1=LR1.predict_proba(Geno_valid_5E4)\n",
    "                AUC_valid=roc_auc_score(disease_valid,Y1[:,1])\n",
    "            elif s==3:\n",
    "                LR1.fit(Geno_train_5E5,disease_train)\n",
    "                Y1=LR1.predict_proba(Geno_valid_5E5)\n",
    "                AUC_valid=roc_auc_score(disease_valid,Y1[:,1])\n",
    "            elif s==4:\n",
    "                LR1.fit(Geno_train_5E6,disease_train)\n",
    "                Y1=LR1.predict_proba(Geno_valid_5E6)\n",
    "                AUC_valid=roc_auc_score(disease_valid,Y1[:,1])\n",
    "            if AUC_valid>AUC_max:\n",
    "                AUC_max=AUC_valid\n",
    "                Cx_max=Cx\n",
    "                s_best=s\n",
    "            \n",
    "    LR2 = AdaBoostClassifier(base_estimator=Cx_max)\n",
    "    AUC_test=0\n",
    "    if s_best==1:\n",
    "        LR2.fit(Geno_train_5E3,disease_train)\n",
    "        Y1=LR2.predict_proba(Geno_valid_5E3)\n",
    "        Y2=LR2.predict_proba(Geno_test_5E3)\n",
    "        AUC_test=roc_auc_score(disease_test,Y2[:,1])\n",
    "        np.save('./ada/'+index+'_valid.npy',Y1)\n",
    "        np.save('./ada/'+index+'_test.npy',Y2)\n",
    "    elif s_best==2:\n",
    "        LR2.fit(Geno_train_5E4,disease_train)\n",
    "        Y1=LR2.predict_proba(Geno_valid_5E4)\n",
    "        Y2=LR2.predict_proba(Geno_test_5E4)\n",
    "        AUC_test=roc_auc_score(disease_test,Y2[:,1])\n",
    "        np.save('./ada/'+index+'_valid.npy',Y1)\n",
    "        np.save('./ada/'+index+'_test.npy',Y2)\n",
    "    elif s_best==3:\n",
    "        LR2.fit(Geno_train_5E5,disease_train)\n",
    "        Y1=LR2.predict_proba(Geno_valid_5E5)\n",
    "        Y2=LR2.predict_proba(Geno_test_5E5)\n",
    "        AUC_test=roc_auc_score(disease_test,Y2[:,1])\n",
    "        np.save('./ada/'+index+'_valid.npy',Y1)\n",
    "        np.save('./ada/'+index+'_test.npy',Y2)\n",
    "    elif s_best==4:\n",
    "        LR2.fit(Geno_train_5E6,disease_train)\n",
    "        Y1=LR2.predict_proba(Geno_valid_5E6)\n",
    "        Y2=LR2.predict_proba(Geno_test_5E6)\n",
    "        AUC_test=roc_auc_score(disease_test,Y2[:,1])\n",
    "        np.save('./ada/'+index+'_valid.npy',Y1)\n",
    "        np.save('./ada/'+index+'_test.npy',Y2)\n",
    "    out=open('./ada/ada_'+index,'w')\n",
    "    out.write(index+'\\t'+str(AUC_max)+'\\t'+str(AUC_test)+'\\n')\n",
    "    out.close() \n",
    "for m in phenotype:\n",
    "    predict(m)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import matplotlib\n",
    "import re\n",
    "import numpy as np\n",
    "import operator\n",
    "from scipy import stats as sta\n",
    "import math\n",
    "from scipy.interpolate import spline\n",
    "import operator\n",
    "from itertools import groupby\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from operator import itemgetter\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from scipy.stats.stats import pearsonr\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn import linear_model\n",
    "from scipy.stats import norm\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import multiprocessing\n",
    "import pickle\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "dir_path1='/oak/stanford/groups/arend/Eric/UKBB/Genenvironment/meta_analysis/data_extraction/train/'\n",
    "dir_path2='/oak/stanford/groups/arend/Eric/UKBB/Genenvironment/meta_analysis/data_extraction/valid/'\n",
    "dir_path3='/oak/stanford/groups/arend/Eric/UKBB/Genenvironment/meta_analysis/data_extraction/test/'\n",
    "phenotype=['1002','1348','1066','1111','1134','1154','1220','1242','1265','1294','1297','1374','1065','1068','1113','1136','1207','1224','1243','1293','1295','1452']\n",
    "def predict(index):\n",
    "    print('loading train')\n",
    "    Geno_train_5E3=np.load(dir_path1+index+'_genotype_train_5E3.npy')\n",
    "    Geno_train_5E4=np.load(dir_path1+index+'_genotype_train_5E4.npy')\n",
    "    Geno_train_5E5=np.load(dir_path1+index+'_genotype_train_5E5.npy')\n",
    "    Geno_train_5E6=np.load(dir_path1+index+'_genotype_train_5E6.npy')\n",
    "    print('loading train')\n",
    "\n",
    "    print('loading valid')\n",
    "    Geno_valid_5E3=np.load(dir_path2+index+'_genotype_valid_5E3.npy')\n",
    "    Geno_valid_5E4=np.load(dir_path2+index+'_genotype_valid_5E4.npy')\n",
    "    Geno_valid_5E5=np.load(dir_path2+index+'_genotype_valid_5E5.npy')\n",
    "    Geno_valid_5E6=np.load(dir_path2+index+'_genotype_valid_5E6.npy')\n",
    "    print('loading valid')\n",
    "\n",
    "    print('loading test')\n",
    "    Geno_test_5E3=np.load(dir_path3+index+'_genotype_test_5E3.npy')\n",
    "    Geno_test_5E4=np.load(dir_path3+index+'_genotype_test_5E4.npy')\n",
    "    Geno_test_5E5=np.load(dir_path3+index+'_genotype_test_5E5.npy')\n",
    "    Geno_test_5E6=np.load(dir_path3+index+'_genotype_test_5E6.npy')\n",
    "    print('loading test')\n",
    "\n",
    "    disease_train=np.load(dir_path1+index+'_disease_train.npy')\n",
    "    disease_valid=np.load(dir_path2+index+'_disease_valid.npy')\n",
    "    disease_test=np.load(dir_path3+index+'_disease_test.npy')\n",
    "    AUC_max=0\n",
    "    Cx_max=0\n",
    "    s_best=0\n",
    "\n",
    "    for s in [1,2,3,4]:\n",
    "        print(s)\n",
    "        for Cx in [0.0001,0.001,0.01,0.1]:\n",
    "            LR1 = GradientBoostingClassifier(min_impurity_decrease=Cx)\n",
    "            AUC_valid=0\n",
    "            if s==1:\n",
    "                LR1.fit(Geno_train_5E3,disease_train)\n",
    "                Y1=LR1.predict_proba(Geno_valid_5E3)\n",
    "                AUC_valid=roc_auc_score(disease_valid,Y1[:,1])\n",
    "            elif s==2:\n",
    "                LR1.fit(Geno_train_5E4,disease_train)\n",
    "                Y1=LR1.predict_proba(Geno_valid_5E4)\n",
    "                AUC_valid=roc_auc_score(disease_valid,Y1[:,1])\n",
    "            elif s==3:\n",
    "                LR1.fit(Geno_train_5E5,disease_train)\n",
    "                Y1=LR1.predict_proba(Geno_valid_5E5)\n",
    "                AUC_valid=roc_auc_score(disease_valid,Y1[:,1])\n",
    "            elif s==4:\n",
    "                LR1.fit(Geno_train_5E6,disease_train)\n",
    "                Y1=LR1.predict_proba(Geno_valid_5E6)\n",
    "                AUC_valid=roc_auc_score(disease_valid,Y1[:,1])\n",
    "            if AUC_valid>AUC_max:\n",
    "                AUC_max=AUC_valid\n",
    "                Cx_max=Cx\n",
    "                s_best=s\n",
    "    LR2 = GradientBoostingClassifier(min_impurity_decrease=Cx_max)\n",
    "    AUC_test=0\n",
    "    if s_best==1:\n",
    "        LR2.fit(Geno_train_5E3,disease_train)\n",
    "        Y1=LR2.predict_proba(Geno_valid_5E3)\n",
    "        Y2=LR2.predict_proba(Geno_test_5E3)\n",
    "        AUC_test=roc_auc_score(disease_test,Y2[:,1])\n",
    "        np.save('./GB/'+index+'_valid.npy',Y1)\n",
    "        np.save('./GB/'+index+'_test.npy',Y2)\n",
    "    elif s_best==2:\n",
    "        LR2.fit(Geno_train_5E4,disease_train)\n",
    "        Y1=LR2.predict_proba(Geno_valid_5E4)\n",
    "        Y2=LR2.predict_proba(Geno_test_5E4)\n",
    "        AUC_test=roc_auc_score(disease_test,Y2[:,1])\n",
    "        np.save('./GB/'+index+'_valid.npy',Y1)\n",
    "        np.save('./GB/'+index+'_test.npy',Y2)\n",
    "    elif s_best==3:\n",
    "        LR2.fit(Geno_train_5E5,disease_train)\n",
    "        Y1=LR2.predict_proba(Geno_valid_5E5)\n",
    "        Y2=LR2.predict_proba(Geno_test_5E5)\n",
    "        AUC_test=roc_auc_score(disease_test,Y2[:,1])\n",
    "        np.save('./GB/'+index+'_valid.npy',Y1)\n",
    "        np.save('./GB/'+index+'_test.npy',Y2)\n",
    "    elif s_best==4:\n",
    "        LR2.fit(Geno_train_5E6,disease_train)\n",
    "        Y1=LR2.predict_proba(Geno_valid_5E6)\n",
    "        Y2=LR2.predict_proba(Geno_test_5E6)\n",
    "        AUC_test=roc_auc_score(disease_test,Y2[:,1])\n",
    "        np.save('./GB/'+index+'_valid.npy',Y1)\n",
    "        np.save('./GB/'+index+'_test.npy',Y2)\n",
    "    out=open('./GB/GB_'+index,'w')\n",
    "    out.write(index+'\\t'+str(AUC_max)+'\\t'+str(AUC_test)+'\\n')\n",
    "    out.close() \n",
    "for m in phenotype:\n",
    "    predict(m)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 19: Disease prediction by L&E only"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lasso Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import matplotlib\n",
    "import re\n",
    "import numpy as np\n",
    "import operator\n",
    "from scipy import stats as sta\n",
    "import math\n",
    "from scipy.interpolate import spline\n",
    "import operator\n",
    "from itertools import groupby\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from operator import itemgetter\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from scipy.stats.stats import pearsonr\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn import linear_model\n",
    "from scipy.stats import norm\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import multiprocessing\n",
    "import pickle\n",
    "\n",
    "dir_path1='/oak/stanford/groups/arend/Eric/UKBB/Genenvironment/meta_analysis/data_extraction/train/'\n",
    "dir_path2='/oak/stanford/groups/arend/Eric/UKBB/Genenvironment/meta_analysis/data_extraction/valid/'\n",
    "dir_path3='/oak/stanford/groups/arend/Eric/UKBB/Genenvironment/meta_analysis/data_extraction/test/'\n",
    "phenotype=['1002','1348','1066','1111','1134','1154','1220','1242','1265','1294','1297','1374','1065','1068','1113','1136','1207','1224','1243','1293','1295','1452']\n",
    "def predict(index):\n",
    "    \n",
    "    Pheno_train=np.load('./train/'+index+'_train_residual.npy')\n",
    "    Pheno_valid=np.load('./valid/'+index+'_valid_residual.npy')\n",
    "    Pheno_test=np.load('./test/'+index+'_test_residual.npy')\n",
    "\n",
    "    disease_train=np.load(dir_path1+index+'_disease_train.npy')\n",
    "    disease_valid=np.load(dir_path2+index+'_disease_valid.npy')\n",
    "    disease_test=np.load(dir_path3+index+'_disease_test.npy')\n",
    "\n",
    "    AUC_max=0\n",
    "    Cx_max=0\n",
    "\n",
    "    for Cx in [0.0001,0.001,0.01,0.1]:\n",
    "        LR1 = LogisticRegression(penalty='l1', C=Cx, max_iter=10000)\n",
    "        AUC_valid=0\n",
    "        LR1.fit(Pheno_train,disease_train)\n",
    "        Y1=LR1.predict_proba(Pheno_valid)\n",
    "        AUC_valid=roc_auc_score(disease_valid,Y1[:,1])\n",
    "        if AUC_valid>AUC_max:\n",
    "            AUC_max=AUC_valid\n",
    "            Cx_max=Cx\n",
    "                    \n",
    "    LR2 = LogisticRegression(penalty='l1', C=Cx_max, max_iter=10000)\n",
    "    AUC_test=0\n",
    "    LR2.fit(Pheno_train,disease_train)\n",
    "    Y2=LR2.predict_proba(Pheno_test)\n",
    "    AUC_test=roc_auc_score(disease_test,Y2[:,1])\n",
    "    np.save('./LR/'+index+'_valid.npy',Y1)\n",
    "    np.save('./LR/'+index+'_test.npy',Y2)\n",
    "    out=open('./LR/LR_'+index,'w')\n",
    "    out.write(index+'\\t'+str(AUC_max)+'\\t'+str(AUC_test)+'\\n')\n",
    "    out.close() \n",
    "for m in phenotype:\n",
    "    print(m)\n",
    "    predict(m)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import matplotlib\n",
    "import re\n",
    "import numpy as np\n",
    "import operator\n",
    "from scipy import stats as sta\n",
    "import math\n",
    "from scipy.interpolate import spline\n",
    "import operator\n",
    "from itertools import groupby\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from operator import itemgetter\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from scipy.stats.stats import pearsonr\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn import linear_model\n",
    "from scipy.stats import norm\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import multiprocessing\n",
    "import pickle\n",
    "from  sklearn.neural_network import MLPClassifier\n",
    "\n",
    "dir_path1='/oak/stanford/groups/arend/Eric/UKBB/Genenvironment/meta_analysis/data_extraction/train/'\n",
    "dir_path2='/oak/stanford/groups/arend/Eric/UKBB/Genenvironment/meta_analysis/data_extraction/valid/'\n",
    "dir_path3='/oak/stanford/groups/arend/Eric/UKBB/Genenvironment/meta_analysis/data_extraction/test/'\n",
    "#phenotype=['1002','1348','1066','1111','1134','1154','1220','1242','1265','1294','1297','1374','1065','1068','1113','1136','1207','1224','1243','1293','1295','1452']\n",
    "phenotype=['1242']\n",
    "def predict(index):\n",
    "    Pheno_train=np.load('./train/'+index+'_train_residual.npy')\n",
    "    Pheno_valid=np.load('./valid/'+index+'_valid_residual.npy')\n",
    "    Pheno_test=np.load('./test/'+index+'_test_residual.npy')\n",
    "\n",
    "    disease_train=np.load(dir_path1+index+'_disease_train.npy')\n",
    "    disease_valid=np.load(dir_path2+index+'_disease_valid.npy')\n",
    "    disease_test=np.load(dir_path3+index+'_disease_test.npy')\n",
    "\n",
    "    AUC_max=0\n",
    "    Cx_max=0\n",
    "\n",
    "    for Cx in [(20,20,20),(30,30),(10,10,10,10),(30,20,10)]:\n",
    "        LR1=MLPClassifier(hidden_layer_sizes=Cx)\n",
    "        AUC_valid=0\n",
    "        LR1.fit(Pheno_train,disease_train)\n",
    "        Y1=LR1.predict_proba(Pheno_valid)\n",
    "        AUC_valid=roc_auc_score(disease_valid,Y1[:,1])\n",
    "        if AUC_valid>AUC_max:\n",
    "            AUC_max=AUC_valid\n",
    "            Cx_max=Cx\n",
    "                    \n",
    "    LR2=MLPClassifier(hidden_layer_sizes=Cx_max)\n",
    "    AUC_test=0\n",
    "    LR2.fit(Pheno_train,disease_train)\n",
    "    Y2=LR2.predict_proba(Pheno_test)\n",
    "    AUC_test=roc_auc_score(disease_test,Y2[:,1])\n",
    "    np.save('./NN/'+index+'_valid.npy',Y1)\n",
    "    np.save('./NN/'+index+'_test.npy',Y2)\n",
    "    out=open('./NN/NN_'+index,'w')\n",
    "    out.write(index+'\\t'+str(AUC_max)+'\\t'+str(AUC_test)+'\\n')\n",
    "    out.close() \n",
    "for m in phenotype:\n",
    "    predict(m)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import matplotlib\n",
    "import re\n",
    "import numpy as np\n",
    "import operator\n",
    "from scipy import stats as sta\n",
    "import math\n",
    "from scipy.interpolate import spline\n",
    "import operator\n",
    "from itertools import groupby\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from operator import itemgetter\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from scipy.stats.stats import pearsonr\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn import linear_model\n",
    "from scipy.stats import norm\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import multiprocessing\n",
    "import pickle\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "dir_path1='/oak/stanford/groups/arend/Eric/UKBB/Genenvironment/meta_analysis/data_extraction/train/'\n",
    "dir_path2='/oak/stanford/groups/arend/Eric/UKBB/Genenvironment/meta_analysis/data_extraction/valid/'\n",
    "dir_path3='/oak/stanford/groups/arend/Eric/UKBB/Genenvironment/meta_analysis/data_extraction/test/'\n",
    "phenotype=['1002','1348','1066','1111','1134','1154','1220','1242','1265','1294','1297','1374','1065','1068','1113','1136','1207','1224','1243','1293','1295','1452']\n",
    "def predict(index):\n",
    "    Pheno_train=np.load('./train/'+index+'_train_residual.npy')\n",
    "    Pheno_valid=np.load('./valid/'+index+'_valid_residual.npy')\n",
    "    Pheno_test=np.load('./test/'+index+'_test_residual.npy')\n",
    "\n",
    "    disease_train=np.load(dir_path1+index+'_disease_train.npy')\n",
    "    disease_valid=np.load(dir_path2+index+'_disease_valid.npy')\n",
    "    disease_test=np.load(dir_path3+index+'_disease_test.npy')\n",
    "\n",
    "    AUC_max=0\n",
    "    Cx_max=0\n",
    "\n",
    "    for Cx in [0.0001,0.001,0.01,0.1]:\n",
    "        LR1 = RandomForestClassifier(min_impurity_decrease=Cx)\n",
    "        AUC_valid=0\n",
    "        LR1.fit(Pheno_train,disease_train)\n",
    "        Y1=LR1.predict_proba(Pheno_valid)\n",
    "        AUC_valid=roc_auc_score(disease_valid,Y1[:,1])\n",
    "        if AUC_valid>AUC_max:\n",
    "            AUC_max=AUC_valid\n",
    "            Cx_max=Cx\n",
    "                    \n",
    "    LR2 = RandomForestClassifier(min_impurity_decrease=Cx_max)\n",
    "    AUC_test=0\n",
    "    LR2.fit(Pheno_train,disease_train)\n",
    "    Y2=LR2.predict_proba(Pheno_test)\n",
    "    AUC_test=roc_auc_score(disease_test,Y2[:,1])\n",
    "    np.save('./RF/'+index+'_valid.npy',Y1)\n",
    "    np.save('./RF/'+index+'_test.npy',Y2)\n",
    "    out=open('./RF/RF_'+index,'w')\n",
    "    out.write(index+'\\t'+str(AUC_max)+'\\t'+str(AUC_test)+'\\n')\n",
    "    out.close() \n",
    "for m in phenotype:\n",
    "    predict(m)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adaboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import matplotlib\n",
    "import re\n",
    "import numpy as np\n",
    "import operator\n",
    "from scipy import stats as sta\n",
    "import math\n",
    "from scipy.interpolate import spline\n",
    "import operator\n",
    "from itertools import groupby\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from operator import itemgetter\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from scipy.stats.stats import pearsonr\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn import linear_model\n",
    "from scipy.stats import norm\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import multiprocessing\n",
    "import pickle\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.tree import ExtraTreeClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "dir_path1='/oak/stanford/groups/arend/Eric/UKBB/Genenvironment/meta_analysis/data_extraction/train/'\n",
    "dir_path2='/oak/stanford/groups/arend/Eric/UKBB/Genenvironment/meta_analysis/data_extraction/valid/'\n",
    "dir_path3='/oak/stanford/groups/arend/Eric/UKBB/Genenvironment/meta_analysis/data_extraction/test/'\n",
    "phenotype=['1002','1348','1066','1111','1134','1154','1220','1242','1265','1294','1297','1374','1065','1068','1113','1136','1207','1224','1243','1293','1295','1452']\n",
    "def predict(index):\n",
    "    \n",
    "    Pheno_train=np.load('./train/'+index+'_train_residual.npy')\n",
    "    Pheno_valid=np.load('./valid/'+index+'_valid_residual.npy')\n",
    "    Pheno_test=np.load('./test/'+index+'_test_residual.npy')\n",
    "\n",
    "    disease_train=np.load(dir_path1+index+'_disease_train.npy')\n",
    "    disease_valid=np.load(dir_path2+index+'_disease_valid.npy')\n",
    "    disease_test=np.load(dir_path3+index+'_disease_test.npy')\n",
    "\n",
    "    AUC_max=0\n",
    "    Cx_max=''\n",
    "\n",
    "    for Cx in [DecisionTreeClassifier(),LogisticRegression(),ExtraTreeClassifier(),GaussianNB()]:\n",
    "        LR1 = AdaBoostClassifier(base_estimator=Cx)\n",
    "        AUC_valid=0\n",
    "        LR1.fit(Pheno_train,disease_train)\n",
    "        Y1=LR1.predict_proba(Pheno_valid)\n",
    "        AUC_valid=roc_auc_score(disease_valid,Y1[:,1])\n",
    "        if AUC_valid>AUC_max:\n",
    "            AUC_max=AUC_valid\n",
    "            Cx_max=Cx\n",
    "                    \n",
    "    LR2 = AdaBoostClassifier(base_estimator=Cx_max)\n",
    "    AUC_test=0\n",
    "    LR2.fit(Pheno_train,disease_train)\n",
    "    Y2=LR2.predict_proba(Pheno_test)\n",
    "    AUC_test=roc_auc_score(disease_test,Y2[:,1])\n",
    "    np.save('./ada/'+index+'_valid.npy',Y1)\n",
    "    np.save('./ada/'+index+'_test.npy',Y2)\n",
    "    out=open('./ada/ada_'+index,'w')\n",
    "    out.write(index+'\\t'+str(AUC_max)+'\\t'+str(AUC_test)+'\\n')\n",
    "    out.close() \n",
    "for m in phenotype:\n",
    "    predict(m)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import matplotlib\n",
    "import re\n",
    "import numpy as np\n",
    "import operator\n",
    "from scipy import stats as sta\n",
    "import math\n",
    "from scipy.interpolate import spline\n",
    "import operator\n",
    "from itertools import groupby\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from operator import itemgetter\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from scipy.stats.stats import pearsonr\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn import linear_model\n",
    "from scipy.stats import norm\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import multiprocessing\n",
    "import pickle\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "dir_path1='/oak/stanford/groups/arend/Eric/UKBB/Genenvironment/meta_analysis/data_extraction/train/'\n",
    "dir_path2='/oak/stanford/groups/arend/Eric/UKBB/Genenvironment/meta_analysis/data_extraction/valid/'\n",
    "dir_path3='/oak/stanford/groups/arend/Eric/UKBB/Genenvironment/meta_analysis/data_extraction/test/'\n",
    "phenotype=['1002','1348','1066','1111','1134','1154','1220','1242','1265','1294','1297','1374','1065','1068','1113','1136','1207','1224','1243','1293','1295','1452']\n",
    "def predict(index):\n",
    "    \n",
    "    Pheno_train=np.load('./train/'+index+'_train_residual.npy')\n",
    "    Pheno_valid=np.load('./valid/'+index+'_valid_residual.npy')\n",
    "    Pheno_test=np.load('./test/'+index+'_test_residual.npy')   \n",
    "\n",
    "    disease_train=np.load(dir_path1+index+'_disease_train.npy')\n",
    "    disease_valid=np.load(dir_path2+index+'_disease_valid.npy')\n",
    "    disease_test=np.load(dir_path3+index+'_disease_test.npy')\n",
    "\n",
    "   \n",
    "    AUC_max=0\n",
    "    Cx_max=0\n",
    "\n",
    "    for Cx in [0.0001,0.001,0.01,0.1]:\n",
    "        LR1 = GradientBoostingClassifier(min_impurity_decrease=Cx)\n",
    "        AUC_valid=0\n",
    "        LR1.fit(Pheno_train,disease_train)\n",
    "        Y1=LR1.predict_proba(Pheno_valid)\n",
    "        AUC_valid=roc_auc_score(disease_valid,Y1[:,1])\n",
    "        if AUC_valid>AUC_max:\n",
    "            AUC_max=AUC_valid\n",
    "            Cx_max=Cx\n",
    "                    \n",
    "    LR2 = GradientBoostingClassifier(min_impurity_decrease=Cx_max)\n",
    "    AUC_test=0\n",
    "    LR2.fit(Pheno_train,disease_train)\n",
    "    Y2=LR2.predict_proba(Pheno_test)\n",
    "    AUC_test=roc_auc_score(disease_test,Y2[:,1])\n",
    "    np.save('./GB/'+index+'_valid.npy',Y1)\n",
    "    np.save('./GB/'+index+'_test.npy',Y2)\n",
    "    out=open('./GB/GB_'+index,'w')\n",
    "    out.write(index+'\\t'+str(AUC_max)+'\\t'+str(AUC_test)+'\\n')\n",
    "    out.close() \n",
    "for m in phenotype:\n",
    "    print(m)\n",
    "    predict(m)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 20: Disease prediction by joint model (score aggregation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import matplotlib\n",
    "import re\n",
    "import statsmodels.api as sm\n",
    "from sklearn.decomposition import PCA\n",
    "import numpy as np\n",
    "import operator\n",
    "from scipy import stats as sta\n",
    "import math\n",
    "from scipy.interpolate import spline\n",
    "import multiprocessing\n",
    "import operator\n",
    "import scipy\n",
    "from itertools import groupby\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from operator import itemgetter\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from scipy.stats.stats import pearsonr\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import cross_val_score\n",
    "def generate(index,out_test):\n",
    "    print(index)   \n",
    "    dir_data0='/oak/stanford/groups/arend/Eric/UKBB/Genenvironment/meta_analysis/data_extraction/valid/'\n",
    "    dir_data1='/oak/stanford/groups/arend/Eric/UKBB/Genenvironment/meta_analysis/data_extraction/test/'\n",
    "    dir_data2='/oak/stanford/groups/arend/Eric/UKBB/Genenvironment/prediction_final/prediction/final/ML_genotype/'\n",
    "    dir_data3='/oak/stanford/groups/arend/Eric/UKBB/Genenvironment/prediction_final/prediction/final/ML_phenotype/remove_age_effect/'\n",
    "    LR_genotype_valid=np.load(dir_data2+'/LR/'+index+'_valid.npy')\n",
    "    NN_genotype_valid=np.load(dir_data2+'/NN/'+index+'_valid.npy')\n",
    "    RF_genotype_valid=np.load(dir_data2+'/RF/'+index+'_valid.npy')\n",
    "    GB_genotype_valid=np.load(dir_data2+'/GB/'+index+'_valid.npy')\n",
    "    ada_genotype_valid=np.load(dir_data2+'/ada/'+index+'_valid.npy')\n",
    "    LR_phenotype_valid=np.load(dir_data3+'/LR/'+index+'_valid.npy')\n",
    "    NN_phenotype_valid=np.load(dir_data3+'/NN/'+index+'_valid.npy')\n",
    "    RF_phenotype_valid=np.load(dir_data3+'/RF/'+index+'_valid.npy')\n",
    "    GB_phenotype_valid=np.load(dir_data3+'/GB/'+index+'_valid.npy')\n",
    "    ada_phenotype_valid=np.load(dir_data3+'/ada/'+index+'_valid.npy')\n",
    "    disease_valid=np.load(dir_data0+index+'_disease_valid.npy')\n",
    "\n",
    "    LR_genotype_test=np.load(dir_data2+'/LR/'+index+'_test.npy')\n",
    "    NN_genotype_test=np.load(dir_data2+'/NN/'+index+'_test.npy')\n",
    "    RF_genotype_test=np.load(dir_data2+'/RF/'+index+'_test.npy')\n",
    "    GB_genotype_test=np.load(dir_data2+'/GB/'+index+'_test.npy')\n",
    "    ada_genotype_test=np.load(dir_data2+'/ada/'+index+'_test.npy')\n",
    "    LR_phenotype_test=np.load(dir_data3+'/LR/'+index+'_test.npy')\n",
    "    NN_phenotype_test=np.load(dir_data3+'/NN/'+index+'_test.npy')\n",
    "    RF_phenotype_test=np.load(dir_data3+'/RF/'+index+'_test.npy')\n",
    "    GB_phenotype_test=np.load(dir_data3+'/GB/'+index+'_test.npy')\n",
    "    ada_phenotype_test=np.load(dir_data3+'/ada/'+index+'_test.npy')\n",
    "    disease_test=np.load(dir_data1+index+'_disease_test.npy')\n",
    "\n",
    "\n",
    "    LR_merge_valid=np.concatenate((LR_genotype_valid,LR_phenotype_valid),axis=1)\n",
    "    NN_merge_valid=np.concatenate((NN_genotype_valid,NN_phenotype_valid),axis=1)\n",
    "    RF_merge_valid=np.concatenate((RF_genotype_valid,RF_phenotype_valid),axis=1)\n",
    "    GB_merge_valid=np.concatenate((GB_genotype_valid,GB_phenotype_valid),axis=1)\n",
    "    ada_merge_valid=np.concatenate((ada_genotype_valid,ada_phenotype_valid),axis=1)\n",
    "\n",
    "    LR_merge_test=np.concatenate((LR_genotype_test,LR_phenotype_test),axis=1)\n",
    "    NN_merge_test=np.concatenate((NN_genotype_test,NN_phenotype_test),axis=1)\n",
    "    RF_merge_test=np.concatenate((RF_genotype_test,RF_phenotype_test),axis=1)\n",
    "    GB_merge_test=np.concatenate((GB_genotype_test,GB_phenotype_test),axis=1)\n",
    "    ada_merge_test=np.concatenate((ada_genotype_test,ada_phenotype_test),axis=1)\n",
    "\n",
    "\n",
    "\n",
    "    LR_LR = LogisticRegression(penalty='l1', C=10000, max_iter=10000)\n",
    "    LR_LR.fit(LR_merge_valid,disease_valid)\n",
    "    Y_LR=LR_LR.predict_proba(LR_merge_test)\n",
    "    AUC_LR=roc_auc_score(disease_test,Y_LR[:,1])\n",
    "    np.save('./LR/'+index+'_valid.npy',Y_LR)\n",
    "    np.save('./LR/'+index+'_test.npy',Y_LR)\n",
    "\n",
    "\n",
    "    LR_NN = LogisticRegression(penalty='l1', C=10000, max_iter=10000)\n",
    "    LR_NN.fit(NN_merge_valid,disease_valid)\n",
    "    Y_NN=LR_NN.predict_proba(NN_merge_test)\n",
    "    AUC_NN=roc_auc_score(disease_test,Y_NN[:,1])\n",
    "    np.save('./NN/'+index+'_valid.npy',Y_NN)\n",
    "    np.save('./NN/'+index+'_test.npy',Y_NN)\n",
    "\n",
    "\n",
    "\n",
    "    LR_RF = LogisticRegression(penalty='l1', C=10000, max_iter=10000)\n",
    "    LR_RF.fit(RF_merge_valid,disease_valid)\n",
    "    Y_RF=LR_RF.predict_proba(RF_merge_test)\n",
    "    AUC_RF=roc_auc_score(disease_test,Y_RF[:,1])\n",
    "    np.save('./RF/'+index+'_valid.npy',Y_RF)\n",
    "    np.save('./RF/'+index+'_test.npy',Y_RF)\n",
    "\n",
    "\n",
    "    LR_GB = LogisticRegression(penalty='l1', C=10000, max_iter=10000)\n",
    "    LR_GB.fit(GB_merge_valid,disease_valid)\n",
    "    Y_GB=LR_GB.predict_proba(GB_merge_test)\n",
    "    AUC_GB=roc_auc_score(disease_test,Y_GB[:,1])\n",
    "    np.save('./GB/'+index+'_valid.npy',Y_GB)\n",
    "    np.save('./GB/'+index+'_test.npy',Y_GB)\n",
    "\n",
    "\n",
    "    LR_ada = LogisticRegression(penalty='l1', C=10000, max_iter=10000)\n",
    "    LR_ada.fit(ada_merge_valid,disease_valid)\n",
    "    Y_ada=LR_ada.predict_proba(ada_merge_test)\n",
    "    AUC_ada=roc_auc_score(disease_test,Y_ada[:,1])\n",
    "    np.save('./ada/'+index+'_valid.npy',Y_ada)\n",
    "    np.save('./ada/'+index+'_test.npy',Y_ada)\n",
    "\n",
    "    out_test.write(str(index)+'\\t'+str(round(AUC_LR,3))+'\\t'+str(round(AUC_RF,3))+'\\t'+str(round(AUC_NN,3))+'\\t'+str(round(AUC_ada,3))+'\\t'+str(round(AUC_GB,3))+'\\n')\n",
    "phenotype=['1002','1348','1066','1111','1134','1154','1220','1242','1265','1294','1297','1374','1065','1068','1113','1136','1207','1224','1243','1293','1295','1452']\n",
    "out_test=open('all_mergescore','w')\n",
    "for index in phenotype:\n",
    "    print(index)\n",
    "    generate(index,out_test)\n",
    "out_test.close()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
